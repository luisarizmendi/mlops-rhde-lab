# PIPELINE DEFINITION
# Name: yolo-training-pipeline
# Description: Pipeline to download data, train YOLO model, and upload results to MinIO
# Inputs:
#    minio_access_key: str
#    minio_bucket: str
#    minio_endpoint: str
#    minio_secret_key: str
#    pvc_name_sufix: str [Default: '-kubeflow-pvc']
#    pvc_size: str [Default: '5Gi']
#    pvc_storage_class: str [Default: 'gp3-csi']
#    roboflow_api_key: str
#    roboflow_project: str
#    roboflow_version: int
#    roboflow_workspace: str
#    train_batch_size: int [Default: 16.0]
#    train_epochs: int [Default: 50.0]
#    train_img_size: int [Default: 640.0]
#    train_name: str [Default: 'yolo']
components:
  comp-createpvc:
    executorLabel: exec-createpvc
    inputDefinitions:
      parameters:
        access_modes:
          description: 'AccessModes to request for the provisioned PVC. May

            be one or more of ``''ReadWriteOnce''``, ``''ReadOnlyMany''``, ``''ReadWriteMany''``,
            or

            ``''ReadWriteOncePod''``. Corresponds to `PersistentVolumeClaim.spec.accessModes
            <https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes>`_.'
          parameterType: LIST
        annotations:
          description: Annotations for the PVC's metadata. Corresponds to `PersistentVolumeClaim.metadata.annotations
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaim>`_.
          isOptional: true
          parameterType: STRUCT
        pvc_name:
          description: 'Name of the PVC. Corresponds to `PersistentVolumeClaim.metadata.name
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaim>`_.
            Only one of ``pvc_name`` and ``pvc_name_suffix`` can

            be provided.'
          isOptional: true
          parameterType: STRING
        pvc_name_suffix:
          description: 'Prefix to use for a dynamically generated name, which

            will take the form ``<argo-workflow-name>-<pvc_name_suffix>``. Only one

            of ``pvc_name`` and ``pvc_name_suffix`` can be provided.'
          isOptional: true
          parameterType: STRING
        size:
          description: The size of storage requested by the PVC that will be provisioned.
            For example, ``'5Gi'``. Corresponds to `PersistentVolumeClaim.spec.resources.requests.storage
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaimSpec>`_.
          parameterType: STRING
        storage_class_name:
          defaultValue: ''
          description: 'Name of StorageClass from which to provision the PV

            to back the PVC. ``None`` indicates to use the cluster''s default

            storage_class_name. Set to ``''''`` for a statically specified PVC.'
          isOptional: true
          parameterType: STRING
        volume_name:
          description: 'Pre-existing PersistentVolume that should back the

            provisioned PersistentVolumeClaim. Used for statically

            specified PV only. Corresponds to `PersistentVolumeClaim.spec.volumeName
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaimSpec>`_.'
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        name:
          parameterType: STRING
  comp-deletepvc:
    executorLabel: exec-deletepvc
    inputDefinitions:
      parameters:
        pvc_name:
          description: Name of the PVC to delete. Supports passing a runtime-generated
            name, such as a name provided by ``kubernetes.CreatePvcOp().outputs['name']``.
          parameterType: STRING
  comp-download-dataset:
    executorLabel: exec-download-dataset
    inputDefinitions:
      parameters:
        api_key:
          parameterType: STRING
        project:
          parameterType: STRING
        version:
          parameterType: NUMBER_INTEGER
        workspace:
          parameterType: STRING
    outputDefinitions:
      parameters:
        dataset_path:
          parameterType: STRING
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      parameters:
        batch_size:
          defaultValue: 16.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataset_path:
          parameterType: STRING
        epochs:
          defaultValue: 50.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        img_size:
          defaultValue: 640.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        name:
          defaultValue: yolo
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        test_dir:
          parameterType: STRING
        train_dir:
          parameterType: STRING
  comp-upload-to-minio:
    executorLabel: exec-upload-to-minio
    inputDefinitions:
      parameters:
        access_key:
          parameterType: STRING
        bucket:
          parameterType: STRING
        endpoint:
          parameterType: STRING
        secret_key:
          parameterType: STRING
        test_dir:
          parameterType: STRING
        train_dir:
          parameterType: STRING
    outputDefinitions:
      parameters:
        model_path:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-createpvc:
      container:
        image: argostub/createpvc
    exec-deletepvc:
      container:
        image: argostub/deletepvc
    exec-download-dataset:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'roboflow' 'pyyaml'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_dataset(\n    api_key: str,\n    workspace: str,\n \
          \   project: str,\n    version: int,\n    dataset_path: dsl.OutputPath(str)\n\
          ) -> None:\n    from roboflow import Roboflow\n    import yaml\n    import\
          \ os\n\n    rf = Roboflow(api_key=api_key)\n    project = rf.workspace(workspace).project(project)\n\
          \    version = project.version(version)\n    dataset = version.download(\"\
          yolov11\")\n\n    # Update data.yaml paths\n    dataset_yaml_path = f\"\
          {dataset.location}/data.yaml\"\n    with open(dataset_yaml_path, \"r\")\
          \ as file:\n        data_config = yaml.safe_load(file)\n\n    data_config[\"\
          train\"] = f\"{dataset.location}/train/images\"\n    data_config[\"val\"\
          ] = f\"{dataset.location}/valid/images\"\n    data_config[\"test\"] = f\"\
          {dataset.location}/test/images\"\n\n    with open(dataset_path, \"w\") as\
          \ f:\n        f.write(dataset.location)\n\n"
        image: quay.io/luisarizmendi/pytorch-custom:latest
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'ultralytics'\
          \ 'torch' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(\n    dataset_path: str,\n    epochs: int = 50,\n\
          \    batch_size: int = 16,\n    img_size: int = 640,\n    name: str = \"\
          yolo\",\n) -> NamedTuple('Outputs', [\n    ('train_dir', str),\n    ('test_dir',\
          \ str)\n]):\n    import torch\n    from ultralytics import YOLO\n    from\
          \ typing import NamedTuple\n    import os\n\n    device = torch.device(\"\
          cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    CONFIG = {\n\
          \        'name': name,\n        'model': 'yolo11m.pt',\n        'data':\
          \ f\"{dataset_path}/data.yaml\",\n        'epochs': epochs,\n        'batch':\
          \ batch_size,\n        'imgsz': img_size,\n        'device': device,\n \
          \   }\n\n    # Configure PyTorch\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"\
          ] = \"expandable_segments:True\"\n\n    # Initialize and train model\n \
          \   model = YOLO(CONFIG['model'])\n    results_train = model.train(\n  \
          \      name=CONFIG['name'],\n        data=CONFIG['data'],\n        epochs=CONFIG['epochs'],\n\
          \        batch=CONFIG['batch'],\n        imgsz=CONFIG['imgsz'],\n      \
          \  device=CONFIG['device'],\n    )\n\n    # Evaluate model\n    results_test\
          \ = model.val(\n        data=CONFIG['data'],\n        split='test',\n  \
          \      device=CONFIG['device'],\n        imgsz=CONFIG['imgsz']\n    )\n\n\
          \    # Export model\n    model.export(format='onnx', imgsz=CONFIG['imgsz'])\n\
          \n\n    return NamedTuple('Outputs', [('train_dir', str), ('test_dir', str)])(\n\
          \        train_dir=str(results_train.save_dir),\n        test_dir=str(results_test.save_dir)\n\
          \    )\n\n"
        image: quay.io/luisarizmendi/pytorch-custom:latest
    exec-upload-to-minio:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_to_minio
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'minio' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_to_minio(\n    train_dir: str,\n    test_dir: str,\n \
          \   endpoint: str,\n    access_key: str,\n    secret_key: str,\n    bucket:\
          \ str,\n    model_path: dsl.OutputPath(str)\n) -> None:\n    from minio\
          \ import Minio\n    from minio.error import S3Error\n    import os\n   \
          \ import datetime\n\n    client = Minio(\n        endpoint.replace('https://',\
          \ '').replace('http://', ''),\n        access_key=access_key,\n        secret_key=secret_key,\n\
          \        secure=True\n    )\n\n    # Get paths for files\n    weights_path\
          \ = os.path.join(train_dir, \"weights\")\n\n    files_train = [os.path.join(train_dir,\
          \ f) for f in os.listdir(train_dir) \n                   if os.path.isfile(os.path.join(train_dir,\
          \ f))]\n    files_models = [os.path.join(weights_path, f) for f in os.listdir(weights_path)\
          \ \n                    if os.path.isfile(os.path.join(weights_path, f))]\n\
          \n    files_model_pt = os.path.join(train_dir, \"weights\") + \"/best.pt\"\
          \n\n    files_model_onnx = os.path.join(train_dir, \"weights\") + \"/best.onnx\"\
          \n\n    files_test = [os.path.join(test_dir, f) for f in os.listdir(test_dir)\
          \ \n                  if os.path.isfile(os.path.join(test_dir, f))]\n\n\
          \    directory_name = os.path.basename(train_dir) + \"-\" + datetime.datetime.now().strftime(\"\
          %Y-%m-%d-%H%M\")\n\n    # Upload files\n    for file_path in files_train:\n\
          \        try:\n            client.fput_object(bucket, \n               \
          \              f\"models/{directory_name}/train-val/{os.path.basename(file_path)}\"\
          , \n                             file_path)\n        except S3Error as e:\n\
          \            print(f\"Error uploading {file_path}: {e}\")\n\n    for file_path\
          \ in files_test:\n        try:\n            client.fput_object(bucket, \n\
          \                             f\"models/{directory_name}/test/{os.path.basename(file_path)}\"\
          , \n                             file_path)\n        except S3Error as e:\n\
          \            print(f\"Error uploading {file_path}: {e}\")\n\n    with open(model_path,\
          \ \"w\") as f:\n        f.write(\"models/\" + directory_name)\n\n    try:\n\
          \        client.fput_object(bucket, \n                        f\"models/{directory_name}/model/pytorch/{os.path.basename(file_path)}\"\
          , \n                        files_model_pt)\n    except S3Error as e:\n\
          \        print(f\"Error uploading {files_model_pt}: {e}\")\n\n    try:\n\
          \        client.fput_object(bucket, \n                        f\"models/{directory_name}/model/onnx/1/{os.path.basename(file_path)}\"\
          , \n                        files_model_onnx)\n    except S3Error as e:\n\
          \        print(f\"Error uploading {files_model_onnx}: {e}\")\n\n"
        image: quay.io/luisarizmendi/pytorch-custom:latest
pipelineInfo:
  description: Pipeline to download data, train YOLO model, and upload results to
    MinIO
  name: yolo-training-pipeline
root:
  dag:
    tasks:
      createpvc:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-createpvc
        inputs:
          parameters:
            access_modes:
              runtimeValue:
                constant:
                - ReadWriteOnce
            pvc_name_suffix:
              componentInputParameter: pvc_name_sufix
            size:
              componentInputParameter: pvc_size
            storage_class_name:
              componentInputParameter: pvc_storage_class
        taskInfo:
          name: createpvc
      deletepvc:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-deletepvc
        dependentTasks:
        - createpvc
        - upload-to-minio
        inputs:
          parameters:
            pvc_name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc
        taskInfo:
          name: deletepvc
      download-dataset:
        cachingOptions: {}
        componentRef:
          name: comp-download-dataset
        dependentTasks:
        - createpvc
        inputs:
          parameters:
            api_key:
              componentInputParameter: roboflow_api_key
            project:
              componentInputParameter: roboflow_project
            version:
              componentInputParameter: roboflow_version
            workspace:
              componentInputParameter: roboflow_workspace
        taskInfo:
          name: download-dataset
      train-model:
        cachingOptions: {}
        componentRef:
          name: comp-train-model
        dependentTasks:
        - createpvc
        - download-dataset
        inputs:
          parameters:
            batch_size:
              componentInputParameter: train_batch_size
            dataset_path:
              taskOutputParameter:
                outputParameterKey: dataset_path
                producerTask: download-dataset
            epochs:
              componentInputParameter: train_epochs
            img_size:
              componentInputParameter: train_img_size
            name:
              componentInputParameter: train_name
        taskInfo:
          name: train-model
      upload-to-minio:
        cachingOptions: {}
        componentRef:
          name: comp-upload-to-minio
        dependentTasks:
        - createpvc
        - train-model
        inputs:
          parameters:
            access_key:
              componentInputParameter: minio_access_key
            bucket:
              componentInputParameter: minio_bucket
            endpoint:
              componentInputParameter: minio_endpoint
            secret_key:
              componentInputParameter: minio_secret_key
            test_dir:
              taskOutputParameter:
                outputParameterKey: test_dir
                producerTask: train-model
            train_dir:
              taskOutputParameter:
                outputParameterKey: train_dir
                producerTask: train-model
        taskInfo:
          name: upload-to-minio
  inputDefinitions:
    parameters:
      minio_access_key:
        parameterType: STRING
      minio_bucket:
        parameterType: STRING
      minio_endpoint:
        parameterType: STRING
      minio_secret_key:
        parameterType: STRING
      pvc_name_sufix:
        defaultValue: -kubeflow-pvc
        isOptional: true
        parameterType: STRING
      pvc_size:
        defaultValue: 5Gi
        isOptional: true
        parameterType: STRING
      pvc_storage_class:
        defaultValue: gp3-csi
        isOptional: true
        parameterType: STRING
      roboflow_api_key:
        parameterType: STRING
      roboflow_project:
        parameterType: STRING
      roboflow_version:
        parameterType: NUMBER_INTEGER
      roboflow_workspace:
        parameterType: STRING
      train_batch_size:
        defaultValue: 16.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      train_epochs:
        defaultValue: 50.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      train_img_size:
        defaultValue: 640.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      train_name:
        defaultValue: yolo
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.11.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-download-dataset:
          pvcMount:
          - mountPath: /opt/app-root/src
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc
        exec-train-model:
          pvcMount:
          - mountPath: /opt/app-root/src
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc
        exec-upload-to-minio:
          pvcMount:
          - mountPath: /opt/app-root/src
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc
