{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Store the Model\n",
    "\n",
    "Save the trained model to the Object Storage system configured in your Workbench connection. \n",
    "\n",
    "Start by getting the credentials and configuring variables for accessing Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from minio import Minio\n",
    "from minio.error import S3Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deserialize from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "results_train_save_path = \"model_train_results.pth\"\n",
    "results_train = torch.load(results_train_save_path)\n",
    "\n",
    "results_test_save_path = \"model_test_results.pth\"\n",
    "results_test = torch.load(results_test_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_ENDPOINT_NAME = os.getenv(\"AWS_S3_ENDPOINT\", \"\").replace('https://', '').replace('http://', '')\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_S3_BUCKET = os.getenv(\"AWS_S3_BUCKET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the S3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Minio(\n",
    "    AWS_S3_ENDPOINT_NAME,\n",
    "    access_key=AWS_ACCESS_KEY_ID,\n",
    "    secret_key=AWS_SECRET_ACCESS_KEY,\n",
    "    secure=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select files to be uploaded (files generated while training and validating the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_train = results_train['save_dir']\n",
    "\n",
    "model_path_train = results_train['save_dir']\n",
    "weights_path = os.path.join(model_path_train, \"weights\")\n",
    "model_path_test = results_test['save_dir']\n",
    "\n",
    "files_train = [os.path.join(model_path_train, f) for f in os.listdir(model_path_train) if os.path.isfile(os.path.join(model_path_train, f))]\n",
    "files_models = [os.path.join(weights_path, f) for f in os.listdir(weights_path) if os.path.isfile(os.path.join(weights_path, f))]\n",
    "files_test = [os.path.join(model_path_test, f) for f in os.listdir(model_path_test) if os.path.isfile(os.path.join(model_path_test, f))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_name= os.path.basename(model_path_train)\n",
    "\n",
    "for file_path_train in files_train:\n",
    "    try:\n",
    "        client.fput_object(AWS_S3_BUCKET, \"prototype/pipeline/\" + directory_name + \"/train-val/\" + os.path.basename(file_path_train), file_path_train)\n",
    "        print(f\"'{os.path.basename(file_path_train)}' is successfully uploaded as object to bucket '{AWS_S3_BUCKET}'.\")\n",
    "    except S3Error as e:\n",
    "        print(\"Error occurred: \", e)\n",
    "\n",
    "for file_path_model in files_models:\n",
    "    try:\n",
    "        client.fput_object(AWS_S3_BUCKET, \"prototype/pipeline/\" + directory_name + \"/\" + os.path.basename(file_path_model), file_path_model)\n",
    "        print(f\"'{os.path.basename(file_path_model)}' is successfully uploaded as object to bucket '{AWS_S3_BUCKET}'.\")\n",
    "    except S3Error as e:\n",
    "        print(\"Error occurred: \", e)\n",
    "\n",
    "for file_path_test in files_test:\n",
    "    try:\n",
    "        client.fput_object(AWS_S3_BUCKET, \"prototype/pipeline/\" + directory_name + \"/test/\" + os.path.basename(file_path_test), file_path_test)\n",
    "        print(f\"'{os.path.basename(file_path_test)}' is successfully uploaded as object to bucket '{AWS_S3_BUCKET}'.\")\n",
    "    except S3Error as e:\n",
    "        print(\"Error occurred: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Remove local files\n",
    "\n",
    "Once you uploaded the Model data to the Object Storage, you can remove the local files to save disk space and also the files where we stored the variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {model_path_train}\n",
    "!rm -rf {model_path_test}\n",
    "!rm -rf {results_train_save_path}\n",
    "!rm -rf {results_test_save_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
