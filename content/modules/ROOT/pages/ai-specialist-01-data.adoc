= Data Management

In the context of AI, data management is a critical foundational phase that determines the overall success of the model. A well-executed data management strategy directly impacts the performance, accuracy, and reliability of the AI system since the AI model learns from data 

It involves three essential steps:

* Data Collection: Gathering and inspecting raw data from various sources, such as video feeds, images, or sensor outputs.

* Data Preparation: Cleaning, augmenting, and labeling data to ensure it's ready for training the AI model.

There are multiple available tools that help with the AI data management. Below you can find a quick comparison of some of the more popular data management tools:

[cols="1,2,2"]
|===
| Tool | Benefits | Drawbacks

| https://roboflow.com/[*Roboflow*]  
| - Simple, intuitive interface  
  - Supports various annotation types (bounding boxes, segmentation, etc.)  
  - Automated augmentation and preprocessing  
  - Direct integration with YOLO and other object detection models  
| - Limited free tier  
  - Requires internet access for cloud-based processing  

| https://github.com/HumanSignal/label-studio[*Label Studio* ] 
| - Flexible and supports multiple data types (image, video, audio, text)  
  - Open-source and self-hosted options available  
  - Collaborative labeling environment  
| - Steeper learning curve compared to Roboflow  
  - Manual setup required for self-hosting  

| https://www.cvat.ai/[*CVAT (Computer Vision Annotation Tool)* ] 
| - Open-source and highly customizable  
  - Supports complex annotation tasks (object tracking, polygons, etc.)  
  - Active community and enterprise support available  
| - Can be resource-intensive  
  - More complex interface for beginners  

| https://www.makesense.ai/[*Makesense.ai*]  
| - Free and web-based  
  - Simple to use for quick annotations  
  - No sign-up required  
| - Lacks advanced features (e.g., automation, augmentation)  
  - Export options are limited  
|===

For this project, https://roboflow.com/[Roboflow] will be the primary tool used for managing and preparing data. Roboflow’s ease of use and direct integration with popular object detection models like YOLO make it the optimal choice for this project.

[example]
====
Your first task will be to create a Roboflow account (if you don't have one already):

1. Go to https://roboflow.com/ and click  `Get Started` in the top-right corner.

2. Choose your preferred sign-up method (such as email) and enter your name and password.

3. You’ll be prompted to create a new workspace, you can choose your preferred name, for example `workshop`. This will serve as the central hub for organizing datasets and projects.

4. (Optional) Go to "Projects" on the left menu and click "+New Project". Give it a name (ie. "group-pass:[<span id="gnumberVal"></span>]-hardhat-detection"), include an annotation group (ie. "hardhats") and select "Object Detection" as `Project Type`. This step is optional because if you fork a Dataset (explain later) it will create a new Project for you. 
====

image::ai-data-roboflow.png[]

[IMPORTANT]

Roboflow includes some restrictions to Not-Paid accounts. The most important for us is that the overall number of images in your account must be less than 10,000, so we need to mantain the size of the Dataset that we will create in the next point below that number. 

From here, you're ready to start...


== Collection

Data collection is the foundational step in AI model development, encompassing the systematic gathering and organization of information that will be used to train and validate machine learning models. This process begins with identifying relevant data sources, which can include both primary data gathered directly through sensors, surveys, or experiments, and secondary data obtained from existing databases, APIs, or public datasets. The quality and scope of collected data directly influence the model's performance, making it crucial to establish robust collection methods and quality control measures from the outset.

In the context of AI development, data collection goes beyond mere gathering of information. It requires careful consideration of data relevance, accuracy, and representativeness to ensure the resulting model can effectively learn and generalize patterns. This involves implementing proper sampling strategies, maintaining consistency in data formats, and ensuring adequate coverage of all relevant scenarios the model might encounter. Additionally, organizations must navigate legal and ethical considerations, including privacy regulations, data ownership rights, and consent requirements.

Successful data collection relies on a well-planned infrastructure that can handle the required data volume while maintaining data integrity. This includes establishing secure storage systems, implementing validation protocols, and ensuring proper documentation of data sources and collection methodologies. The collected data must be regularly monitored for quality and relevance, with mechanisms in place to identify and address any biases or gaps that could impact model performance. Through careful attention to these aspects, organizations can build a solid foundation for their AI development projects and maximize the likelihood of creating effective, reliable models.

In our case, you will focusing only in the data gathering more than in the data storage and access management, since we will be using the Roboflow tool that will help with those point in this specific case.

We will be using the YOLO (You Only Look Once) object detection model. YOLO requires that the data that we use to be labeled to effectively train and identify objects within an image. This aligns with the principles of supervised learning, where the model learns from labeled datasets to predict outcomes accurately, contrast with the Unsupervised or Reinforcement learning:

* Supervised Learning: Models are trained on labeled data (e.g., images with hardhat/no hardhat labels). This ensures the model can classify and detect objects correctly. This is our current use case.

* Unsupervised Learning: Models work with unlabeled data, identifying patterns or clusters without predefined categories.

* Reinforcement Learning: The model learns through trial and error by receiving feedback based on its actions.


When collecting data you can perform a manual data gathering, by capturing new images/videos of the objects that you want to detect, but that's often only done when no suitable datasets are publicly available. While gathering raw data is essential, labeling data is often the most time-consuming part of the process. Annotating thousands of images manually can introduce bottlenecks and require significant human effort. Already created pre-labeled datasets streamline the model development process by providing ready-to-use data, allowing you to focus on model tuning and experimentation. That's why, when possible (not special data is needed), you should prioritize gathering data from datasets that are already annotated to accelerate the AI development lifecycle. 

There are multiple ways to get pre-Labeled image datasets, for example:

* Private Datasets: You can pay to be allowed to use certain private Datasets already created or you can hire a 3rd party to create your own specific Dataset

* Public Datasets: Many datasets are freely available for computer vision tasks. Examples include https://cocodataset.org/#home[COCO] (Common Objects in Context) or the https://storage.googleapis.com/openimages/web/index.html[Open Images Dataset].

* Open Data Repositories: Platforms like https://www.kaggle.com/datasets[Kaggle], https://datasetsearch.research.google.com/[Google Dataset Search], and https://universe.roboflow.com/[Roboflow Universe] provide datasets contributed by the community.

* Synthetic Data: Using AI or simulation tools to generate artificial but realistic datasets. This approach is useful when real-world data is scarce or expensive to obtain.

In our project, we will be using pre-labeled data from the https://universe.roboflow.com/[Roboflow Universe] using the already created account in Roboflow. We will use the Datasets that you find in Roboflow Universe to add images to your own "Project" in your Roboflow account.


=== Dataset Search 

Roboflow Universe hosts a vast collection of datasets, including both original contributions and replicated datasets. When selecting the appropriate dataset for your project, the key considerations are finding relevant labels and ensuring sufficient image quantity.
When it comes to dataset size, the general principle is "the more, the better." AI model performance typically shows a direct correlation with the volume of training data available. The larger and more diverse your dataset, the better your model can learn and generalize patterns.

[CAUTION]

With a free Roboflow account, you're limited to 10,000 images per account.

For hardhat detection specifically, you'll want to focus on datasets with labels such as `hardhat` or `helmet`. However, it's crucial to understand that effective safety compliance detection requires a balanced approach. You need to identify both when workers are wearing hardhats and when they're not. This means your dataset should include images labeled with `no-hardhat` or similar tags to identify non-compliance scenarios. This dual approach ensures your model can effectively distinguish between compliant and non-compliant situations, making it more reliable for real-world safety monitoring.

[example]
====
Now that you know what to look for, pick the source Datasets that you will be using in your project:

1. Go to https://universe.roboflow.com/[Roboflow Universe] 

2. Select "*Object Detection*" in the `By Project Type` filter. This is required since other types of vision ai projects won't include required labeled data, for example the object classification does not include the location of the object.
 
3. Identify one or multiple datasets with relevant labeled data by playing with the "Advanced Filters". You can add `class:<name>` into the search box to only show datasets that contains data with the 'name' label, for example `class:hardhat`.
====

image::ai-data-datasets.png[]


Reaching the optimal dataset size of 10,000 images often requires combining multiple datasets from Roboflow Universe. While the platform offers an "Image Count" filter, be cautious when using it as your sole metric. This filter displays the total number of images in a dataset, not the count of images containing your specific labels of interest, which could lead to misleading results.


[example]
====
To accurately determine the number of relevant tags in images in a specific dataset, follow these steps:

1. Navigate to the dataset's URL in Roboflow Universe
2. Click the "Images" button
3. Use the Filter function to select a single target Class (label)
4. Check the pagination counter at the bottom of the page, which displays the total count (for example, 1 - 50 of 75)
5. Repeat for other classes
====

[NOTE]

When you select multiple classes you will be applying an "AND" operator so the result will show only images where both classes appear at the same time.


image::ai-data-image-count.png[]


Beyond the image count, it's essential to verify that both images and labels align with your specific use case. For instance, when detecting "helmets" in industrial environments, images of people cycling wearing "helmets" would be inappropriate for your dataset. Dataset image inspection is crucial before implementation, as including irrelevant images could significantly skew your model's predictions.


Once you have choosen your source Datasets, take note of their Roboflow Universe URLs since you will need them in the next step. 


[TIP]
====
If you don't find appropiate source Datasets you can use this one:   

https://universe.roboflow.com/pped/pped-batch1
====


=== Image Gathering 

Now you need to create your own Dataset out of the labeled images of the source Dataset/s. In order to do that you have two options: you can fork an entire Dataset in your account, or you can clone certain specific images only. 


==== Fork Dataset 

When you fork a Dataset you "copy" it into your account. This is useful if you found a single Dataset that is similar to what you are looking for and you don't need to choose few images from multiple different Datasets.

If you selected multiple Datasets in your search, you start by forking the one that is closer to what you need and then Clone images from additional Datasets later.

Also forking is useful if you encounter issues while cloning images since forking typically results in fewer issues than cloning in Roboflow. Even if an error appears, the images will still be copied to your account.


[example]
====
If you want to fork a Dataset follow these steps:

1. Navigate to the dataset's URL in Roboflow Universe
2. Click the "Images" button
3. Click the "Fork Dataset" button
4. Confirm and wait until fork is done
5. Optionally, rename the Project in your account (Fork keeps the original name) by selecting the option when you clik on the three dots.

====


==== Cloning Images 

Sometimes cloning the images with the required labels makes more sense than forking an entire Dataset, or you want to add more images into your already forked Dataset.

[example]
====
To clone a subset of images in a Dataset you have to:

1. Navigate to the dataset's URL in Roboflow Universe
2. Click the "Images" button
3. Use the Filter function to select your target Class (labels)
4. Click the box right above the first image to select all images

[NOTE]
Probably the Dataset will have more than 50 images that you want to clone. You can go page by page selecting all images but it's a better idea to show all images in a single page before clicking the selection box. In order to do that look in the URL line for the variable `pageSize=50` and change it to the number of images that you want to clone, for example `https://universe.roboflow.com/pped/pped-batch1/browse?queryText=class%3Ahelmet&`*pageSize=3500*`&startingIndex=0&browseQuery=true`

5. Check that all images are selected and then click "Clone <number> Selected Images" on the top right corner. Select the Workspace and the Project that you created before and click "Clone <number> Images"

[NOTE]
If the page does not respond or you find errors, try to clone images in batches of 900 images instead of performing a single clone with a high number of items.
====

image::ai-data-clone.png[]

Repeat these steps for each class in each of your selected source Datasets until you have a balanced dataset with an overall image number close to 7,000 or 8,000 items (leave space to include a new label later)


=== Manual inputs 

If you have time and energy, you can try to load new images and perform the labeling on your own, to experience and have an idea of the effort that it takes to annotate a full Dataset.


[example]
====
In order to upload new images you have to: 


1. Navigate to the Project's URL in your Roboflow account
2. Click the "Upload Data" on the left menu
3. Select your images
4. Click "Save and Continue"
====

Once the images are uploaded Roboflow gives you three options to annotate (add labels) your images: Auto Label (Roboflow automation), Maunal Labeling and Roboflow Labeling (hire Roboflow people to label your images). In our case we will proceed with Manual Labeling.

[example]
====
Once you have assigned images to be annotated, you can follow these steps:


1. Navigate to the Project's URL in your Roboflow account
2. Click the "Annotate" on the left menu
3. Click "Start Annotating" in the top right corner
4. Make a selection in the image and assign a class
5. Repeat for each label on each image...
6. Go back to the "Annotate" page and click "Submit for Review" on top right corner
7. Since you are the only one in your Project, you can click on the "Review" column where the new images will appear
8. Select images and start Approving or Rejecting the labeling
9. Once done, go back to the Annotate page and click "Add Appoved to Dataset" on top right
10. Click "Add Images"
====


image::ai-data-annotate.png[]



== Preparation

blah, blah









As mentioned, when you fork you copy all images. Probably the source Dataset had labels that you don't need in your Project, so it's better to remove those labels and images.

[example]
====
After forking, ensure you remove any images that don't belong to the hardhat or no_hardhat classes. To remove unwanted images:

1. Go to the "Dataset" section in the left menu.
2. Select the class you want to delete and be sure that the your classes are not (click on the `x`, see the image below).
3. Use the option to display all images.
4. Select all images by checking the box at the top.
5. Choose the "Remove from Project" action.

This will help clean the dataset, keeping only the relevant classes.
====

image::ai-data-removeimages.png[]























Visual AI models can take various approaches depending on the objective:

* Object Classification: Determines the type of object in an image but does not specify its location.

* Object Detection: Identifies and localizes objects within an image.

* Segmentation: Divides the image into segments, classifying each pixel into different object categories.

* Pose Estimation: Tracks and identifies the key points or joints of objects (typically used for human posture recognition).

* Object Tracking: Follows objects across frames in a video, maintaining their identity over time. Useful for surveillance or autonomous driving.

* Action Recognition: Classifies actions happening in videos by analyzing sequences of frames, widely used in video surveillance and human activity recognition.

* Anomaly Detection: Identifies unusual patterns in visual data, often used for defect detection in manufacturing.

Since object detection is the focus, as the goal is to identify workers wearing hardhats and detect their locations in the image.






Unlike object classification, object detection requires detailed annotations that highlight the exact location of the object within the image. This involves:

* Drawing bounding boxes around the target objects (e.g., hardhats).

* Assigning labels to each box (e.g., 'hardhat' or 'no hardhat').

* Ensuring consistency and accuracy across the entire dataset.

