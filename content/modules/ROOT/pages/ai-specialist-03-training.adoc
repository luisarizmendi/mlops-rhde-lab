= Model Training

In production environments, training machine learning models is not as simple as running a script or experimenting in a notebook. A robust pipeline is essential to ensure:

* Scalability: Handle large datasets and distribute computational workloads across resources efficiently.
* Reproducibility: Replicate training runs for consistent model performance.
* Automation: Automate steps like data preprocessing, model training, evaluation, and deployment, reducing manual errors.
* Monitoring and Governance: Track metrics, manage versions, and ensure compliance and performance.

Without a pipeline, the training process becomes ad hoc and error-prone, leading to inconsistent results and increased operational risks.

Elyra Pipelines, built on Jupyter Notebooks, offer an accessible way to prototype workflows. However, they lack the scalability, integrations, and enterprise-grade features required for production. For example:

* Inefficiency with large-scale data or distributed workloads.
* Tight dependency on Jupyter Notebook, unsuitable for high-availability systems.
* Limited monitoring tools, storage integrations, and governance capabilities.

While Elyra is excellent for prototyping, Kubeflow Pipelines stand out as a robust, production-ready solution. Built for Kubernetes, Kubeflow supports the entire machine learning lifecycle—from data preparation to deployment—with features like:

* Seamless scalability and fault tolerance.
* Integration with CI/CD platforms, monitoring tools, and storage systems.
* Modular pipelines with version control and strong community support.

For enterprises seeking a scalable MLOps solution, Kubeflow on OpenShift AI is a powerful choice.



=== Tools and preparations

We’ll use the following tools to train a YOLO model to detect hardhats:

* OpenShift AI: Manage Kubeflow pipelines within a Kubernetes-based environment. Leverage the Model Registry to store, version, and track models, along with their metadata (e.g., training parameters and metrics).

* GitHub: Version control and collaborative development for pipeline definitions.

* MinIO: Scalable, high-performance object storage for model outputs.

These tools are pre-configured at this point, allowing you to dive straight into pipeline development.


[IMPORTANT]

You must have a Pipeline Server defined to proceed. If this step was completed during the Elyra Pipelines setup, you're all set. However, if you skipped that part, you’ll need to follow the steps to create the Pipeline Server now before continuing.



== Kubeflow Pipeline 


=== Creating the Training Pipeline 

The training pipeline is implemented using Python. OpenShift AI Workbenches provide a coding environment with an integrated IDE. 


[example]
====
Let’s deploy a Code Workbench to get started:

1- Deploy the Workbench: Go to `OpenShift AI > Data Science Projects`, and select your project. 

2- In the `Workbenches` tab, click `Create Workbench`. Name it (e.g., "Object Detection Pipeline Code") and choose the `code-server` type. Keep the default local volume; no need to configure Object Storage or GPUs. Click `Create Workbench` and wait for the deployment to complete.

3- Access the Workbench: Once the Workbench is ready, open it. 

4- From the menu (three horizontal lines, top-left), select `Terminal > New Terminal`.

image::ai-train-code-terminal.png[]

5- Clone the Repository: Use the terminal to clone your GitHub repository:

[source,shell,role=execute,subs="attributes"]
----
git clone http://{gitea-server}/user<span id="gnumberVal"></span>/user<span id="gnumberVal"></span>-ai.git
----

6. Set Up the Pipeline Script: Create a new file `hardhat-kubeflow-pipeline.py` in the cloned directory (right-click to create a file). Copy the example script from: https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/training/kubeflow/hardhat-kubeflow-pipeline.py[hardhat-kubeflow-pipeline.py]. Save it and prepare for review.
====

Next, we’ll review the script’s implementation to understand the pipeline’s logic and adapt it to our needs.

Let's review the https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/training/kubeflow/hardhat-kubeflow-pipeline.py[`hardhat-kubeflow-pipeline.py` script]. 

The script defines the pipeline structure, specifying the order in which pipeline components are executed. It also includes the input variable definitions and the Kubernetes resources allocated to each task. Similar to the Elyra pipeline, the steps are distinct, meaning that if variables or files are required from one step in another, they must be explicitly 'passed' between them. Kubeflow simplifies this process by offering tools to https://www.kubeflow.org/docs/components/pipelines/user-guides/data-handling/[manage data handling between tasks] (there is also a feature to https://www.kubeflow.org/docs/components/pipelines/user-guides/data-handling/artifacts/[pass ML artifacts] that is not used in this workshop), allowing you to easily define input and output variables for seamless step-to-step communication. Additionally, Kubeflow provides straightforward mechanisms for creating and attaching Kubernetes Persistent Volumes, enabling file sharing between tasks. For instance, the dataset downloaded by the initial task can be stored on a volume and subsequently accessed by the training task without manual intervention.

[NOTE]

The `train` step has an additional Persistent Volumen mounted to https://stackoverflow.com/questions/43373463/how-to-increase-shm-size-of-a-kubernetes-container-shm-size-equivalent-of-doc[extend the POD's Shared Memory]. This is needed when you run the inference using GPUs. Kubeflow v2 https://github.com/kubeflow/pipelines/pull/10913[already implemented the method to use `EmptyDir`] instead but the current version of the OpenShift AI pipelines does not have this path yet, so a Persistent Volume is used instead.






[source,python,role=execute,subs="attributes"]
----
# Define the pipeline
@dsl.pipeline(
    name='YOLO Training Pipeline',
    description='Pipeline to download data, train YOLO model, and upload results to MinIO'
)
def yolo_training_pipeline_shm(

    roboflow_api_key: str,
    roboflow_workspace: str,
    roboflow_project: str,
    roboflow_version: int,
    minio_endpoint: str,
    minio_access_key: str,
    minio_secret_key: str,
    minio_bucket: str,
    pvc_storage_class: str = "gp3-csi",
    pvc_size: str = "5Gi",
    pvc_name_suffix: str = "-kubeflow-pvc",
    train_name: str = "hardhat",
    train_epochs: int = 50,
    train_batch_size: int = 16,
    train_img_size: int = 640,
    model_registry_name: str = "object-detection-model-registry"
):
        
    from datetime import datetime
    
    # Create PV
    pvc = kubernetes.CreatePVC(
        pvc_name_suffix=pvc_name_suffix,
        access_modes=['ReadWriteOnce'],
        size=pvc_size,
        storage_class_name=pvc_storage_class,
    )
    pvc_shm = kubernetes.CreatePVC(
        pvc_name_suffix="shm",
        access_modes=['ReadWriteOnce'],
        size=pvc_size,
        storage_class_name=pvc_storage_class,
    )    

    # Download dataset
    download_task = download_dataset(
        api_key=roboflow_api_key,
        workspace=roboflow_workspace,
        project=roboflow_project,
        version=roboflow_version
    )
    download_task.set_caching_options(enable_caching=False)
    kubernetes.mount_pvc(
        download_task,
        pvc_name=pvc.outputs['name'],
        mount_path='/opt/app-root/src',
    )


    # Train model
    train_task = train_model(
        dataset_path=download_task.output,
        epochs=train_epochs,
        batch_size=train_batch_size,
        img_size=train_img_size,
        name=train_name
    ).after(download_task)
    train_task.set_memory_request('2Gi')
    train_task.set_memory_limit('4Gi')
    train_task.set_caching_options(enable_caching=False)
    kubernetes.mount_pvc(
        train_task,
        pvc_name=pvc.outputs['name'],
        mount_path='/opt/app-root/src',
    )
    kubernetes.mount_pvc(
        train_task,
        pvc_name=pvc_shm.outputs['name'],
        mount_path='/dev/shm',
    )

    # Upload results
    upload_task = upload_to_minio(
        train_dir=train_task.outputs['train_dir'],
        test_dir=train_task.outputs['test_dir'],
        endpoint=minio_endpoint,
        access_key=minio_access_key,
        secret_key=minio_secret_key,
        bucket=minio_bucket
    ).after(train_task)
    upload_task.set_caching_options(enable_caching=False)
    kubernetes.mount_pvc(
        upload_task,
        pvc_name=pvc.outputs['name'],
        mount_path='/opt/app-root/src',
    )

    delete_pvc = kubernetes.DeletePVC(
        pvc_name=pvc.outputs['name']
    ).after(upload_task)
    
    delete_pvc_shm = kubernetes.DeletePVC(
        pvc_name=pvc_shm.outputs['name']
    ).after(train_task)

    
    # Push to model registry
    push_to_model_registry(
        model_name=train_name,
        version="",
        metrics=train_task.outputs['metrics'],
        model_registry_name=model_registry_name,
        model_artifact_s3_path=upload_task.outputs['model_artifact_s3_path'],
        s3_endpoint=minio_endpoint,
        roboflow_workspace=roboflow_workspace,
        roboflow_project=roboflow_project,
        roboflow_version=roboflow_version,
        train_epochs=train_epochs,
        train_batch_size=train_batch_size,
        train_img_size=train_img_size
    ).after(upload_task)

----

Besides the Pipeline definition, the  https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/training/kubeflow/hardhat-kubeflow-pipeline.py[`hardhat-kubeflow-pipeline.py` script] also contains the step (components) definitions.

The first step is to download the Dataset. This task has a Persistent Volume attached where it will store the Dataset contents. It will use the Roboflow libraries and the provided input variables to download the files directly from Roboflow, as it was done during the Model Development section.

An important aspect to highlight is that each step in the pipeline specifies the base container image to be used. In this case, the custom image `quay.io/luisarizmendi/pytorch-custom:latest` is utilized. This image https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/training/Containerfile[includes all the necessary dependencies], eliminating the need to download and install packages every time the task runs. Additionally, the task definitions include examples of how to install additional packages if needed. However, these package installations won't be executed in this setup, as all required packages are already pre-installed within the custom image, ensuring efficiency and consistency.

[source,python,role=execute,subs="attributes"]
----
# Component 1: Download Dataset
@dsl.component(
    base_image="quay.io/luisarizmendi/pytorch-custom:latest",
    packages_to_install=["roboflow", "pyyaml"]
)
def download_dataset(
    api_key: str,
    workspace: str,
    project: str,
    version: int,
    dataset_path: dsl.OutputPath(str)
) -> None:
    from roboflow import Roboflow
    import yaml
    import os

    rf = Roboflow(api_key=api_key)
    project = rf.workspace(workspace).project(project)
    version = project.version(version)
    dataset = version.download("yolov11")

    # Update data.yaml paths
    dataset_yaml_path = f"{dataset.location}/data.yaml"
    with open(dataset_yaml_path, "r") as file:
        data_config = yaml.safe_load(file)

    data_config["train"] = f"{dataset.location}/train/images"
    data_config["val"] = f"{dataset.location}/valid/images"
    data_config["test"] = f"{dataset.location}/test/images"

    print(dataset)

    with open(dataset_path, "w") as f:
        f.write(dataset.location)
----

After downloading the dataset, the pipeline moves on to the model training task. This task utilizes the same Persistent Volume as the previous step, ensuring seamless access to the dataset files. During this phase, the provided inputs are used to configure the training hyperparameters.

One significant enhancement in this Kubeflow step, compared to the Elyra pipelines, is the calculation of metrics during training. These metrics are stored in a variable and will later be used to populate the metadata in the Model Registry, adding an extra layer of insight and traceability to the model lifecycle.

[source,python,role=execute,subs="attributes"]
----
# Component 2: Train Model
@dsl.component(
    base_image="quay.io/luisarizmendi/pytorch-custom:latest",
    packages_to_install=["ultralytics", "torch", "pandas"]
)
def train_model(
    dataset_path: str,
    epochs: int = 50,
    batch_size: int = 16,
    img_size: int = 640,
    name: str = "yolo",
) -> NamedTuple('Outputs', [
    ('train_dir', str),
    ('test_dir', str),
    ('metrics', dict)
]):
    import torch
    from ultralytics import YOLO
    import pandas as pd
    import os

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    CONFIG = {
        'name': name,
        'model': 'yolo11m.pt',
        'data': f"{dataset_path}/data.yaml",
        'epochs': epochs,
        'batch': batch_size,
        'imgsz': img_size,
        'device': device,
    }

    # Configure PyTorch
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    # Initialize and train model
    model = YOLO(CONFIG['model'])
    results_train = model.train(
        name=CONFIG['name'],
        data=CONFIG['data'],
        epochs=CONFIG['epochs'],
        batch=CONFIG['batch'],
        imgsz=CONFIG['imgsz'],
        device=CONFIG['device'],
    )

    # Evaluate model
    results_test = model.val(
        data=CONFIG['data'],
        split='test',
        device=CONFIG['device'],
        imgsz=CONFIG['imgsz']
    )

    # Compute metrics from CSV
    results_csv_path = os.path.join(results_train.save_dir, "results.csv")
    results_df = pd.read_csv(results_csv_path)

    # Extract metrics
    metrics = {
        "precision": results_df["metrics/precision(B)"].iloc[-1],
        "recall": results_df["metrics/recall(B)"].iloc[-1],
        "mAP50": results_df["metrics/mAP50(B)"].iloc[-1],
        "mAP50-95": results_df["metrics/mAP50-95(B)"].iloc[-1]
    }

    return NamedTuple('Outputs', [
        ('train_dir', str),
        ('test_dir', str),
        ('metrics', dict)
    ])(
        train_dir=str(results_train.save_dir),
        test_dir=str(results_test.save_dir),
        metrics=metrics
    )
----

The training step also shares the Persistent Volume with the next step, as the trained model needs to be uploaded to Object Storage. Since each training iteration may result in a different name and file path, the training name is passed as an input variable. This allows dynamic calculation of the model paths and the performance result files.

The trained model native PyTorch `.pt` is uploaded to the bucket specified by the input variable, stored in the `/model/pytorch` directory. If you have exported the model in alternative formats, such as `onnx` or `torchscript`, you can upload those files as well. However, it's important to consider that certain model serving solutions, like the Model Server, may require a specific directory structure. For instance, OpenVINO serving with the `onnx-1` format expects the directory structure to be `<version>/<model_name>.onnx`, while NVIDIA Triton for `torchscript` expects the format `<version>/model.pt`.

[source,python,role=execute,subs="attributes"]
----
# Component 3: Upload to MinIO
@dsl.component(
    base_image="quay.io/luisarizmendi/pytorch-custom:latest",
    packages_to_install=["minio"]
)
def upload_to_minio(
    train_dir: str,
    test_dir: str,
    endpoint: str,
    access_key: str,
    secret_key: str,
    bucket: str,
    model_path: dsl.OutputPath(str)
) -> NamedTuple('Outputs', [
    ('model_artifact_s3_path', str),
    ('files_model_pt', str)
]):
    from minio import Minio
    from minio.error import S3Error
    import os
    import datetime

    client = Minio(
        endpoint.replace('https://', '').replace('http://', ''),
        access_key=access_key,
        secret_key=secret_key,
        secure=True
    )

    # Get paths for files
    weights_path = os.path.join(train_dir, "weights")

    files_train = [os.path.join(train_dir, f) for f in os.listdir(train_dir)
                   if os.path.isfile(os.path.join(train_dir, f))]
    files_models = [os.path.join(weights_path, f) for f in os.listdir(weights_path)
                    if os.path.isfile(os.path.join(weights_path, f))]

    files_model_pt = os.path.join(train_dir, "weights") + "/best.pt"
    
    #files_model_onnx = os.path.join(train_dir, "weights") + "/best.onnx"
    #files_model_torchscript = os.path.join(train_dir, "weights") + "/best.torchscript"
    
    files_test = [os.path.join(test_dir, f) for f in os.listdir(test_dir) 
                  if os.path.isfile(os.path.join(test_dir, f))]
    
    directory_name = os.path.basename(train_dir) + "-" + datetime.datetime.now().strftime("%Y-%m-%d-%H%M")
    
    # Upload files
    for file_path in files_train:
        try:
            client.fput_object(bucket, f"models/{directory_name}/train-val/{os.path.basename(file_path)}", file_path)
        except S3Error as e:
            print(f"Error uploading {file_path}: {e}")
    
    for file_path in files_test:
        try:
            client.fput_object(bucket, f"models/{directory_name}/test/{os.path.basename(file_path)}", file_path)
        except S3Error as e:
            print(f"Error uploading {file_path}: {e}")

    with open(model_path, "w") as f:
        f.write("models/" + directory_name)

    try:
        client.fput_object(bucket, f"models/{directory_name}/model/pytorch/{os.path.basename(files_model_pt)}", files_model_pt)
    except S3Error as e:
        print(f"Error uploading {files_model_pt}: {e}")

    #try:
    #    client.fput_object(bucket, f"models/{directory_name}/model/onnx/1/{os.path.basename(files_model_onnx)}", files_model_onnx)
    #except S3Error as e:
    #    print(f"Error uploading {files_model_onnx}: {e}")

    #try:
    #    client.fput_object(bucket, f"models/{directory_name}/model/torchscript/1/model.pt", files_model_torchscript)
    #except S3Error as e:
    #    print(f"Error uploading {files_model_torchscript}: {e}")


    model_artifact_s3_path=f"models/{directory_name}/model/pytorch/{os.path.basename(files_model_pt)}"

    return NamedTuple('Outputs', [
        ('model_artifact_s3_path', str),
        ('files_model_pt', str)
    ])(
        model_artifact_s3_path,
        files_model_pt
    )
----

Once the model has been uploaded to Object Storage, the final step is to register the model in the Model Registry.

The OpenShift AI Model Registry is a centralized repository for storing, managing, and tracking machine learning models throughout their lifecycle. It allows you to add rich metadata to each model's description, which can include performance metrics, hyperparameters, model version, and even the container image used for training. This metadata is invaluable for model versioning, auditing, and traceability, ensuring that all relevant information about the model's origin and performance is easily accessible and well-documented for future use.

[source,python,role=execute,subs="attributes"]
----
# Component 4: Push to Model Registry
@dsl.component(
    base_image='python:3.9',
    packages_to_install=['model-registry']
)
def push_to_model_registry(
    model_name: str,
    version: str,
    metrics: dict,
    model_registry_name: str,
    model_artifact_s3_path: str,
    s3_endpoint: str,
    roboflow_workspace: str,
    roboflow_project: str,
    roboflow_version: int,
    train_epochs: int,
    train_batch_size: int,
    train_img_size: int
):
    from model_registry import ModelRegistry
    import os
    from datetime import datetime
    import json
    
    s3_endpoint_url=s3_endpoint.replace('https://', '').replace('http://', '')
    version = version if version else datetime.now().strftime('%y%m%d%H%M')
    model_object_prefix = model_name if model_name else "model"
    cluster_domain= ""    
    namespace_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'
    with open(namespace_file_path, 'r') as namespace_file:
        namespace = namespace_file.read().strip()


    # Get Cluster domain from MinIO s3_endpoint.
    cluster_domain = s3_endpoint.split("//")[-1].split(".", 2)[-1]

 
    os.environ["KF_PIPELINES_SA_TOKEN_PATH"] = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      
   
    def _register_model(author_name , server_address, model_object_prefix, version):
        registry = ModelRegistry(server_address=server_address, port=443, author=author_name, is_secure=False)
        registered_model_name = model_object_prefix
        version_name = version
        metadata = {
            "Dataset": f"https://universe.roboflow.com/{roboflow_workspace}/{roboflow_project}/dataset/{str(roboflow_version)}",
            "Epochs": str(train_epochs),
            "Batch Size": str(train_batch_size),
            "Image Size": str(train_img_size),
            "mAP50": str(metrics["mAP50"]),
            "mAP50-95": str(metrics["mAP50-95"]),
            "precision": str(metrics["precision"]),
            "recall": str(metrics["recall"])
        }
      
        rm = registry.register_model(
            registered_model_name,
            f"s3://{s3_endpoint_url}/{model_artifact_s3_path}",
            model_format_name="pt",
            model_format_version="1",
            version=version_name,
            description=f"{registered_model_name} is a dense neural network that detects Hardhats in images.",
            metadata=metadata
        )
        print("Model registered successfully")
    
    
    # Register the model
    server_address = f"https://{model_registry_name}-rest.apps.{cluster_domain}"
    print(f"s3://{s3_endpoint_url}/{model_artifact_s3_path}")

    _register_model(namespace, server_address, model_object_prefix, f"{model_object_prefix}-{version}")
----


=== Importing the Training Pipeline 

Before proceeding with the import, we need to convert the Python script into a YAML Kubeflow Pipeline definition. Using the `kfp` library, we will execute the script to generate the YAML file required for importing the pipeline

[example]
====
Let's generate the Pipeline YAML file and push it into Gitea.

1. Run the following commands in the Code terminal:

[source,shell,role=execute,subs="attributes"]
----
pip install --upgrade pip
pip install kfp[kubernetes]
cd user<span id="gnumberVal"></span>-ai
python hardhat-kubeflow-pipeline.py
----


2. You will generate a file named `yolo_training_pipeline.yaml`. Next, push the newly created files to Gitea. In the terminal window, run the following commands:

[source,shell,role=execute,subs="attributes"]
----
git config --global user.email user<span id="gnumberVal"></span>@acme.com
git config --global user.name user<span id="gnumberVal"></span>
git remote set-url origin http://user<span id="gnumberVal"></span>:redhat<span id="gnumberVal"></span>@{gitea-server}/user<span id="gnumberVal"></span>/user<span id="gnumberVal"></span>-ai.git
git add .
git commit -m "kubeflow Pipeline"
git push
----

3. Now, navigate to Gitea at http://{gitea-server} to confirm that your files have been successfully pushed. Open the `yolo_training_pipeline.yaml` file and select the Raw option from the top-right menu. Copy the URL of the raw file, as you will need it to import the pipeline.

image::ai-train-gitea-raw.png[]

====

Once you have the YAML file available in Gitea, you can import it directly into OpenShift AI.

[example]
====
To proceed with the Kubeflow Pipeline import:

1. Go to Data Science Pipelines
2. Click Import Pipeline
3. Fill in Name (`hardhat-training`)
4. Select "Import by URL" and include the Gitea URL with the `yolo_training_pipeline.yaml` raw content.

image::ai-train-pipeline-kubeflow-import.png[]

====

After the correct import, you will see the Pipeline diagram:


image::ai-train-kubeflow-pipe.png[]



=== Running the Training Pipeline 

[example]
====
It's time to run the imported Kubeflow Pipeline:

1. Click Actions and then `Create run`
2. Click "Create new experiment" (`hardhat-detection`)
3. Give the run a name (e.g. `v1`)
4. Fill in the environment variables used in your run:
    * Access Key: "userpass:[<span id="gnumberVal"></span>]"
    * Secret Key: "redhatpass:[<span id="gnumberVal"></span>]"
    * Bucket: "userpass:[<span id="gnumberVal"></span>]-ai-models"
    * Endpoint: {minio-api}
    * Model Registry Name: `object-detection-model-registry`
    * PVC sufix: `-kubeflow-pvc`
    * Roboflow API
    * Roboflow Project 
    * Roboflow Workspace 
    * Roboflow version 
    * Batch Size 
    * Ephoch number 
    * Image Size: `640`
    * Training name (e.g. `hardhat`)
====

image::ai-train-pipeline-run.png[]


[NOTE]

In contrast to Elyra Pipelines, this Kubeflow Pipeline automatically creates and deletes the Persistent Volume used for transferring files between pipeline tasks, ensuring efficient resource management and streamlined execution.

[TIP]

Keep in mind that if you're short on time and can't wait for a full model training process, you can opt for the so-called '*MockTrain*' by using the reduced dataset you prepared during the Model Development section and configuring only one epoch. However, note that models trained on this dataset won't be suitable for deployment, as they won't achieve accurate object detection. Instead, you'll need to use the https://github.com/luisarizmendi/workshop-object-detection-rhde/tree/main/resources/solutions/ai-specialist/assets/object-detection-hardhat-or-hat[provided pre-trained model] during the Deployment section to ensure proper functionality.

You can view the details of each task while it's running to monitor important information. Additionally, you can check the POD name generated for the task (top right corner, in a red square in the image below), which is useful for accessing real-time logs in the OpenShift Console (since the Logs tab in the OpenShift AI Pipeline view is only available once the task has completed). You can also track the inputs and outputs associated with each task, which will be displayed once the task finishes, providing insight into the data flow and results at each stage of execution.

image::ai-train-pipeline-pod-task.png[]

You can also open the OpenShift Console and check how the Persistent Volume was created and bounded into the PODs that are used to run the Pipeline tasks.

After some time, the pipeline will finish. You can at that point go to the Object Storage and check the contents that have been uplaoded to it.

image::ai-train-minio.png[]

Additionally, you can check the newly trained model in the Model Registry, where it will be available along with all the associated metadata details that were added during the registration process.

The Model Registry serves as the central hub for model publication. From here, you can directly deploy the model to the same OpenShift cluster running OpenShift AI, utilizing one of the supported Model Serving options. However, in this workshop, we won't be using this method. Instead, model inference will be performed at the Edge using Red Hat Enterprise Linux. More details on this approach will be provided in the xref:ai-specialist-04-deploy.adoc[Model Serving] section.


image::ai-train-registry.png[]


One last thing to mention about the Pipelines: In this example, you ran the pipeline manually. However, in a production environment, you might want to run it periodically. This can be achieved using the Schedule feature, which allows you to automate the execution of your pipeline at defined intervals, ensuring that model training and updates are performed regularly without manual intervention.


image::ai-train-schedule.png[]


== Solution and Next Steps

In this section, a new Kubeflow Pipeline was created based on the https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/training/kubeflow/hardhat-kubeflow-pipeline.py[`hardhat-kubeflow-pipeline.py` script]. This pipeline successfully trained the YOLO model for hardhat detection, generated the model file, and uploaded it into the MinIO Object Storage. This model file will be essential for the next section, xref:ai-specialist-04-deploy.adoc[Model Serving].

However, if you used a reduced dataset ("*MockTrain*") or limited epochs for faster training and believe the model might not perform adequately in the field, it is advisable to discard the generated model. Instead, you can upload and use the  https://github.com/luisarizmendi/workshop-object-detection-rhde/tree/main/resources/solutions/ai-specialist/assets/object-detection-hardhat-or-hat[provided pre-trained model] going forward to ensure better accuracy and robustness in deployment.





