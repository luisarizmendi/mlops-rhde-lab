= Model Training


* OpenShift Pipelines: OpenShift Pipelines is a CI/CD solution for automating workflows, including model training, serving, and monitoring. In the building context, this feature will be used to automate the retrieval, training and containerization of then model. Pipelines are also useful to help with model serving (when deploying on OpenShift) and to periodically update models based on feedback or retraining needs (explained in xref:ai-specialist-04-update.adoc[Day-2 Operations] section).




















https://github.com/opendatahub-io/ai-edge










=== Tools and preparations

In this section we will be using the following tools: 

* OpenShift AI

* GitHub

* MinIO

* Quay.io




openshift ai with rhel


1. Model Registry

Overview

The OpenShift AI Model Registry serves as a central repository for storing, sharing, versioning, and tracking machine learning models. It supports metadata management, including training details, hyperparameters, and performance metrics.

Usage with RHEL Inference

Models can be retrieved from the registry and exported to RHEL for inference.

Use S3-compatible tools or the oc CLI to download model artifacts.

The model metadata can inform deployment strategies and runtime configuration on RHEL.

2. Model Serving

Overview

OpenShift AI provides robust serving platforms to deploy models as inference endpoints:

Single-model serving (KServe): Designed for large models requiring dedicated resources.

Multi-model serving (ModelMesh): Optimized for small and medium-sized models sharing resources.

Usage with RHEL Inference

Containerize models using OpenShift AI’s serving runtime (e.g., NVIDIA Triton, TensorFlow Serving).

Push containerized models to a container registry for deployment on RHEL.

Use Podman or Docker on RHEL to deploy and serve the model locally.

3. Monitoring with TrustyAI

Overview

TrustyAI in OpenShift AI enables monitoring of machine learning models for fairness, bias, and drift. It integrates with OpenShift’s monitoring stack to provide insights into model behavior and performance.

Usage with RHEL Inference

Metrics Collection:

Use Prometheus and Node Exporter on RHEL to collect system and inference metrics.

Forward these metrics to OpenShift’s Prometheus instance.

Bias and Drift Analysis:

Send inference inputs and outputs from RHEL to OpenShift AI for analysis using TrustyAI.

Leverage TrustyAI’s bias and drift metrics for ongoing model evaluation.








1. Model Registry

Overview

The OpenShift AI Model Registry serves as a central repository for storing, sharing, versioning, and tracking machine learning models. It supports metadata management, including training details, hyperparameters, and performance metrics.

Usage with RHEL Inference

Models can be retrieved from the registry and exported to RHEL for inference.

Use S3-compatible tools or the oc CLI to download model artifacts.

The model metadata can inform deployment strategies and runtime configuration on RHEL.

2. Model Serving

Overview

OpenShift AI provides robust serving platforms to deploy models as inference endpoints:

Single-model serving (KServe): Designed for large models requiring dedicated resources.

Multi-model serving (ModelMesh): Optimized for small and medium-sized models sharing resources.

Usage with RHEL Inference

Containerize models using OpenShift AI’s serving runtime (e.g., NVIDIA Triton, TensorFlow Serving).

Push containerized models to a container registry for deployment on RHEL.

Use Podman or Docker on RHEL to deploy and serve the model locally.

3. Monitoring with TrustyAI

Overview

TrustyAI in OpenShift AI enables monitoring of machine learning models for fairness, bias, and drift. It integrates with OpenShift’s monitoring stack to provide insights into model behavior and performance.

Usage with RHEL Inference

Metrics Collection:

Use Prometheus and Node Exporter on RHEL to collect system and inference metrics.

Forward these metrics to OpenShift’s Prometheus instance.

Bias and Drift Analysis:

Send inference inputs and outputs from RHEL to OpenShift AI for analysis using TrustyAI.

Leverage TrustyAI’s bias and drift metrics for ongoing model evaluation.







9. Prometheus and Grafana

Overview

Prometheus and Grafana are integral to OpenShift AI’s monitoring stack, providing metrics collection, visualization, and alerting.

Usage with RHEL Inference

Configure Prometheus on RHEL to expose metrics (e.g., inference latency, resource usage).

Forward these metrics to OpenShift AI’s monitoring stack for centralized visualization and analysis in Grafana.

10. Trust Management

Overview

OpenShift AI ensures secure deployment and serving of models with trust management capabilities, including custom certificates and token-based authentication.

Usage with RHEL Inference

Secure inference endpoints on RHEL using certificates and authentication tokens generated in OpenShift AI.

Integrate secure communication channels between RHEL inference servers and OpenShift AI monitoring systems.












blah, blah

== Packaging

blah, blah

== Serving

blah, blah











-------------------------------------------------------------------------------------------------------------------

MODEL REGISTRY


https://github.com/opendatahub-io/model-registry-operator






















-------------------------------------------------------------------------------------------------------------------












SCHEDULE!!!!!!!











*********************************+
KUBEFLOW PIPELINES


support: https://access.redhat.com/articles/rhoai-supported-configs




https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html-single/openshift_ai_tutorial_-_fraud_detection_example/index#running-a-pipeline-generated-from-python-code





https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html-single/working_with_data_science_pipelines/index#defining-a-pipeline_ds-pipelines

The Kubeflow Pipelines SDK enables you to define end-to-end machine learning and data pipelines. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. After defining the pipeline, you can import the YAML file to the OpenShift AI dashboard to enable you to configure its execution settings.




nuevo workbench code ****





 https://github.com/Ark-kun/kfp_samples/blob/65a98da/2019-10%20Kubeflow%20summit/104%20-%20Passing%20data%20for%20python%20components/104%20-%20Passing%20data%20for%20python%20component



 https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/manipulate-resources/




https://github.com/kubeflow/pipelines/tree/master/samples/core


https://www.kubeflow.org/docs/components/pipelines/user-guides/data-handling/






Crear un nuevo experiment antes de lanzar el run






explicar que se crea un objeto "Workflow" con el pipe run