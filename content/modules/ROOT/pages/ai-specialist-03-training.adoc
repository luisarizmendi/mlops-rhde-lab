= Model Training

In production environments, training machine learning models requires more than just running a script or experimenting in a notebook. A robust pipeline provides the following benefits:

* Scalability: Handle large datasets and distribute computational workloads efficiently across resources.

* Reproducibility: Ensure that every training run can be replicated, enabling consistent model behavior.

* Automation: Reduce manual intervention by automating steps such as data preprocessing, model training, evaluation, and deployment.

* Monitoring and Governance: Track metrics, manage versions, and audit changes to ensure compliance and performance.

Without a pipeline, training a model becomes an ad-hoc, error-prone process, which can lead to inconsistent outcomes and increased operational risks.

Elyra Pipelines, built on Jupyter Notebooks, offer a simple and accessible way to construct data science workflows. However, they fall short when used in production environments for several reasons. Elyra workflows are not designed to efficiently handle large-scale data or distributed workloads across clusters. They are heavily tied to the Jupyter Notebook ecosystem, which is not ideal for enterprise-grade, high-availability systems. Additionally, Elyra lacks advanced integrations with monitoring tools, production-grade storage solutions, and other enterprise MLOps requirements. Managing versions of pipelines and models is cumbersome in Elyra, leading to challenges in reproducibility and governance. While Elyra is a good tool for prototyping and early-stage experimentation, it is not built to meet the demands of a production-scale MLOps pipeline.

Kubeflow, a comprehensive open-source MLOps platform, is specifically designed to address the challenges of deploying machine learning workflows at scale. Built to leverage Kubernetes, it enables seamless scalability, resource management, and fault tolerance. Kubeflow supports the entire machine learning lifecycle, from data preparation to training, tuning, and deployment. It allows integration with enterprise tools such as monitoring systems, CI/CD platforms, and storage solutions. Kubeflow Pipelines provide tools to create, share, and reuse modular pipelines with version control. With strong community support and continuous development, it remains up-to-date with the latest advancements in AI/ML. For enterprises seeking a production-grade MLOps solution, Kubeflow is a highly recommended choice.

OpenShift AI simplifies the deployment and management of AI/ML workloads on Red Hat OpenShift, and it offers built-in support for importing and running Kubeflow Pipelines and that's what we are going to do now.


=== Tools and preparations

In this section, we will utilize the following tools to streamline our workflow:

* OpenShift AI: This platform will be used to import and execute the Kubeflow pipeline, enabling seamless integration with our Kubernetes-based environment. Besides the Pipeline management feature of OpenShift AI, you will be using the "Model Registry". The OpenShift AI Model Registry serves as a central repository for storing, sharing, versioning, and tracking machine learning models. It supports metadata management, including training details, hyperparameters, and performance metrics.

* GitHub: Serving as the repository for the pipeline definition, GitHub ensures version control and collaborative development.

* MinIO: Acting as the storage solution for the produced model, MinIO provides high-performance, scalable object storage.

All tools have been pre-configured and are ready for immediate use, allowing us to dive directly into the tasks ahead without additional setup.



== Training Pipeline

The Kubeflow pipeline that we are going to import is based on Python code. OpenShift AI Workbenches include an environment specificlly created to develop code with an integrated IDE, so let's deploy a new Workbench of that type.

To deploy a Code Workbench you have to:

1. Navigate to OpenShift AI "Data Science Projects" and select your project.
2. Go to "Workbenches" tab and click "Create Wrokbench"
3. Give it a name (e.g. "Object Detection Pipeline Code") and select the `code-server` type. Keep the local volume but you don't need to configure the Object Storage Connection. There is no need for Accelerators even if you have GPUs available.
4. Create the Workbench and wait until is ready to open it.
5. Click on the Three horizontal lines on the top left menu, then select "Terminal" > "New Terminal"


image::ai-train-code-terminal.png[]


6. Use the Terminal Console to clone your Gitea repo: `git clone {gitea}/userpass:[<span id="gnumberVal"></span>]/userpass:[<span id="gnumberVal"></span>]-ai.git`


Now you can create a new file `kubeflow-pipeline.py` (right click on the cloned directory name) and start coding...

In order to make it easy, you can simply copy this file https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/training/kubeflow/kubeflow-pipeline.py and save it.

Let's review the script. 




zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz



pass data between tasks: https://www.kubeflow.org/docs/components/pipelines/user-guides/data-handling/






onnx path format needs a version








zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz




Let's proceed with the import, but first you need to convert the Python script into a YAML Kubeflow Pipeline definition by executing the script thanks to the `kpf` library we will obtain the yaml file that we can use to import the pipeline.



Run the following commands in the Code terminal:

pip install --upgrade pip
pip install kfp[kubernetes]
cd userpass:[<span id="gnumberVal"></span>]-ai
python kubeflow-pipeline.py



You will obtain a file `yolo_training_pipeline.yaml`. It's time to Push the new files into Gitea, on the Terminal window run the following commands:




git config --global user.email userpass:[<span id="gnumberVal"></span>]@acme.com
git config --global user.name userpass:[<span id="gnumberVal"></span>]
git remote set-url origin http://userpass:[<span id="gnumberVal"></span>]:redhatpass:[<span id="gnumberVal"></span>]@{gitea-server}/userpass:[<span id="gnumberVal"></span>]/userpass:[<span id="gnumberVal"></span>]-ai.git
git add .
git commit -m "kubeflow Pipeline"
git push





Now you can go to Gitea (http://{gitea-server}) and check that your files have been pushed. Open the `yolo_training_pipeline.yaml` file and open the `Raw` version (top right menu), you will need that URL to import the pipeline.





image::ai-train-gitea-raw.png[]





To proceed with the Kubeflow Pipeline import:

1. Go to Data Science Pipelines
2. Click Import Pipeline
3. Fill in Name (yolo-training)
4. Select "Import by URL" and include the Gitea URL with the `yolo_training_pipeline.yaml` raw content.



image::ai-train-pipeline-kubeflow-import.png[]


After the correct import, you will see the Pipeline diagram:



image::ai-train-kubeflow-pipe.png[]


It's time to run the imported Kubeflow Pipeline:

Click Actions and then "Create run"
Click "Create new experiment" (hardhat-detection)
Give the run a name (e.g. `v1`)
Fill in the environment variables used in your run:
    * Access Key: "userpass:[<span id="gnumberVal"></span>]"
    * Secret Key: "redhatpass:[<span id="gnumberVal"></span>]"
    * Bucket: "userpass:[<span id="gnumberVal"></span>]-ai-models"
    * Endpoint: {}
    * PVC sufix: 
    * Roboflow API 
    * Roboflow Project 
    * Roboflow Workspace 
    * Roboflow version 
    * Batch Size 
    * Ephoch number 
    * Image Size 
    * Training name (e.g. `hardhat-detection`)




IF THE FIRST RUN FAILS, RE RUN IT 




Plan B?????????????







image::ai-train-pipeline-run.png[]

[NOTE]

In contrast with what happened with the Elyra Pipelines, this Kubeflow Pipeline automataically created and delete the Persistent Volume used to pass the files between Pipeline tasks






se crea pod 
yolo-training-pipeline-d6qhq-system-container-impl-4214525703


pueden ver el pvc claim bond 





explicar que se crea un objeto "Workflow" con el pipe run








check in object storage








SCHEDULE!!!!!!!














-------------------------------------------------------------------------------------------------------------------










== Model Publishing









Go to Model Registry
Click Register model 
Fill in: 
    NAme: hardhat-detection
    Version name: PyTorch v1 
    Source model format: PyTorch  ???? (decir lo de onxx)
    Source model format version: ???? 
    Object Storage: Autofill
    Add the PAth to the model: something like "models/yolo-2025-01-22-1106/best.pt)"
    Click Register model

ai-train-register-model.png


ADD onnx too (Register new Version)



METER LA REGIÓN EN LA CONXIÓN DE OBJECT STORAGE PORQUE SI NO:


!!!
{"code":13, "message":"Failed to pull model from storage due to error: unable to list objects in bucket 'user99-ai-models': MissingRegion: could not find region configuration"}









