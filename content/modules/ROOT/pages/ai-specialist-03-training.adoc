= Model Training


* OpenShift Pipelines: OpenShift Pipelines is a CI/CD solution for automating workflows, including model training, serving, and monitoring. In the building context, this feature will be used to automate the retrieval, training and containerization of then model. Pipelines are also useful to help with model serving (when deploying on OpenShift) and to periodically update models based on feedback or retraining needs (explained in xref:ai-specialist-04-update.adoc[Day-2 Operations] section).




















https://github.com/opendatahub-io/ai-edge










=== Tools and preparations

In this section we will be using the following tools: 

* OpenShift AI

* GitHub

* MinIO

* Quay.io




openshift ai with rhel


1. Model Registry

Overview

The OpenShift AI Model Registry serves as a central repository for storing, sharing, versioning, and tracking machine learning models. It supports metadata management, including training details, hyperparameters, and performance metrics.

Usage with RHEL Inference

Models can be retrieved from the registry and exported to RHEL for inference.

Use S3-compatible tools or the oc CLI to download model artifacts.

The model metadata can inform deployment strategies and runtime configuration on RHEL.

2. Model Serving

Overview

OpenShift AI provides robust serving platforms to deploy models as inference endpoints:

Single-model serving (KServe): Designed for large models requiring dedicated resources.

Multi-model serving (ModelMesh): Optimized for small and medium-sized models sharing resources.

Usage with RHEL Inference

Containerize models using OpenShift AI’s serving runtime (e.g., NVIDIA Triton, TensorFlow Serving).

Push containerized models to a container registry for deployment on RHEL.

Use Podman or Docker on RHEL to deploy and serve the model locally.

3. Monitoring with TrustyAI

Overview

TrustyAI in OpenShift AI enables monitoring of machine learning models for fairness, bias, and drift. It integrates with OpenShift’s monitoring stack to provide insights into model behavior and performance.

Usage with RHEL Inference

Metrics Collection:

Use Prometheus and Node Exporter on RHEL to collect system and inference metrics.

Forward these metrics to OpenShift’s Prometheus instance.

Bias and Drift Analysis:

Send inference inputs and outputs from RHEL to OpenShift AI for analysis using TrustyAI.

Leverage TrustyAI’s bias and drift metrics for ongoing model evaluation.








1. Model Registry

Overview

The OpenShift AI Model Registry serves as a central repository for storing, sharing, versioning, and tracking machine learning models. It supports metadata management, including training details, hyperparameters, and performance metrics.

Usage with RHEL Inference

Models can be retrieved from the registry and exported to RHEL for inference.

Use S3-compatible tools or the oc CLI to download model artifacts.

The model metadata can inform deployment strategies and runtime configuration on RHEL.

2. Model Serving

Overview

OpenShift AI provides robust serving platforms to deploy models as inference endpoints:

Single-model serving (KServe): Designed for large models requiring dedicated resources.

Multi-model serving (ModelMesh): Optimized for small and medium-sized models sharing resources.

Usage with RHEL Inference

Containerize models using OpenShift AI’s serving runtime (e.g., NVIDIA Triton, TensorFlow Serving).

Push containerized models to a container registry for deployment on RHEL.

Use Podman or Docker on RHEL to deploy and serve the model locally.

3. Monitoring with TrustyAI

Overview

TrustyAI in OpenShift AI enables monitoring of machine learning models for fairness, bias, and drift. It integrates with OpenShift’s monitoring stack to provide insights into model behavior and performance.

Usage with RHEL Inference

Metrics Collection:

Use Prometheus and Node Exporter on RHEL to collect system and inference metrics.

Forward these metrics to OpenShift’s Prometheus instance.

Bias and Drift Analysis:

Send inference inputs and outputs from RHEL to OpenShift AI for analysis using TrustyAI.

Leverage TrustyAI’s bias and drift metrics for ongoing model evaluation.







9. Prometheus and Grafana

Overview

Prometheus and Grafana are integral to OpenShift AI’s monitoring stack, providing metrics collection, visualization, and alerting.

Usage with RHEL Inference

Configure Prometheus on RHEL to expose metrics (e.g., inference latency, resource usage).

Forward these metrics to OpenShift AI’s monitoring stack for centralized visualization and analysis in Grafana.

10. Trust Management

Overview

OpenShift AI ensures secure deployment and serving of models with trust management capabilities, including custom certificates and token-based authentication.

Usage with RHEL Inference

Secure inference endpoints on RHEL using certificates and authentication tokens generated in OpenShift AI.

Integrate secure communication channels between RHEL inference servers and OpenShift AI monitoring systems.












blah, blah

== Packaging

blah, blah

== Serving

blah, blah











-------------------------------------------------------------------------------------------------------------------

MODEL REGISTRY






















-------------------------------------------------------------------------------------------------------------------










-------------


create base container image



https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html/managing_openshift_ai/creating-custom-workbench-images#creating-a-custom-image-from-default-image_custom-images



podman build -t quay.io/luisarizmendi/pytorch-custom:latest .






ai-train-custom-image.png





***************************
add it to custom runtime images
***************************

quay.io/luisarizmendi/pytorch-custom:latest


ai-train-custom-elyra-image.png








----

create pipeline server (region none and bucket userxx-ai-pipelines)


ai-train-pipeline-server.png




restart workbench



open workbench

click the gear on the left menu (Runtimes) and then check runtime configuration was loaded automatically











click on "+" tab on the header (close to protyping.ipynb  "x") 

Select "Pipeline Editor" en the Elyra section

Rename .pipeline file "training.pipeline"

















-----

Prepare step files


+ remove pip installs

+ dataset_location from get_data to train (pickle) 

* include environment variables (to get dataset, custom typical hyperparameters), for example 
    'epochs': os.getenv("MODEL_EPOCHS"),
    'batch': os.getenv("MODEL_BATCH"), 



* include imports in each step


+ cambiar path save en el object store

+ create cleanup to remove local files

+ Store the variables from training into a file to pass between pipeline tasks



TRAIN:

results_train_serializable = {
    "maps": results_train.maps,
    "names": results_train.names,
    "save_dir": results_train.save_dir,
    "results_dict": results_train.results_dict,
}

results_train_save_path = "model_train_results.pth"

torch.save(results_train_serializable, results_train_save_path)


results_test_serializable = {
    "maps": results_test.maps,
    "names": results_test.names,
    "save_dir": results_test.save_dir,
    "results_dict": results_test.results_dict,
}

results_test_save_path = "model_test_results.pth"

torch.save(results_test_serializable, results_test_save_path)

    





SVAE:

import torch 

results_train_save_path = "model_train_results.pth"
results_train = torch.load(results_train_save_path)

results_test_save_path = "model_test_results.pth"
results_test = torch.load(results_test_save_path)



y cambiar esto:

model_path_train = results_train['save_dir']

model_path_train = results_train['save_dir']
weights_path = os.path.join(model_path_train, "weights")
model_path_test = results_test['save_dir']

files_train = [os.path.join(model_path_train, f) for f in os.listdir(model_path_train) if os.path.isfile(os.path.join(model_path_train, f))]
files_models = [os.path.join(weights_path, f) for f in os.listdir(weights_path) if os.path.isfile(os.path.join(weights_path, f))]
files_test = [os.path.join(model_path_test, f) for f in os.listdir(model_path_test) if os.path.isfile(os.path.join(model_path_test, f))]



----

drag drop and connect 





click top right square to open the pipeline configuration panel on the right (do not close it we will use it)


for each node: 

+ choose runtime and GPU config (GPU=1 GPU vendor nvidia.com/gpu)

+ Add environment variables  (??? maybe better at the pipeline level instead?)




ADD VOLUME 
* create PVC  (training-pipeline with 5G is enough in the userxx-ai project)
* Config pipeline volume (/opt/app-root/src)







save

run 



------


Go to Experiments and se it running 




tip 
you can review live logs by jumping into openshift and see pod logs 


once the tasks are finishing, you can check the logs 


Check the Object Storage












SCHEDULE!!!!!!!











*********************************+
KUBEFLOW PIPELINES


support: https://access.redhat.com/articles/rhoai-supported-configs




https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html-single/openshift_ai_tutorial_-_fraud_detection_example/index#running-a-pipeline-generated-from-python-code





https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html-single/working_with_data_science_pipelines/index#defining-a-pipeline_ds-pipelines

The Kubeflow Pipelines SDK enables you to define end-to-end machine learning and data pipelines. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. After defining the pipeline, you can import the YAML file to the OpenShift AI dashboard to enable you to configure its execution settings.




