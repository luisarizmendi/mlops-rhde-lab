= Model Release
















































== Serving










2. Model Serving

Overview

OpenShift AI provides robust serving platforms to deploy models as inference endpoints:

Single-model serving (KServe): Designed for large models requiring dedicated resources.

Multi-model serving (ModelMesh): Optimized for small and medium-sized models sharing resources.

Usage with RHEL Inference

Containerize models using OpenShift AI’s serving runtime (e.g., NVIDIA Triton, TensorFlow Serving).

Push containerized models to a container registry for deployment on RHEL.

Use Podman or Docker on RHEL to deploy and serve the model locally.













app info 







== Live Testing















== Workflow Overview

Find below the workflow that you will follow as Application Developer during the workshop (each of those will be a different "module").

image::dev-workflow.png[]

1. *App Planning*: Architecture Design: This foundational phase focuses on system design decisions, technology stack selection, and establishing the technical approach. It sets the blueprint for the entire application development lifecycle.

2. *App Development*: This encompasses the core development activities:

    * Code Development: Writing application code following established design patterns and best practices. This involves implementing features and functionality according to requirements.
    * Testing: Comprehensive testing. This phase often requires iteration back to code development to address identified issues.

3. *App Release*: Integration, Deployment: After successful testing, the application is prepared for production, involving integration with other systems and services, deployment through CI/CD pipelines,final verification in staging environments and production rollout

4. *Day-2 Operations*: Monitoring, Tuning: Post-deployment activities focus on application performance monitoring, resource utilization optimization and performance tuning based on real world usage

This represents a continuous cycle rather than a linear process, with several feedback loops driving ongoing improvement:

    * From operations back to architecture design: Performance metrics and operational insights inform architectural decisions and improvements
    * From operations to development: Production issues may require code changes or optimizations
    * From testing back to development: Issues found during testing lead to code refinement
    * From deployment experiences back to architecture: Real-world deployment challenges might necessitate architectural adjustments

The feedback loops ensure continuous improvement and adaptation to changing requirements, performance needs, and operational realities. For successful application development, it's crucial to understand that you may need to revisit earlier phases as new information or challenges emerge.

Now that you understand your requirements, your task and the workflow that you should follow, you can jump directly into the first module: xref:app-developer-01-arch.adoc[App Planning].













You have been chosen as the lead application developer for a critical safety initiative at ACME Your role is pivotal in building a suite of modern applications that will integrate cutting edge AI models developed by your colleagues to prevent accidents on the factory floor. As you step into this role, the challenge ahead is not just about coding, it’s about designing and deploying software that will safeguard lives and transform workplace safety.

ACME is embracing a "modern application development" approach, signaling a shift towards technologies and architectures that enable greater scalability, flexibility, and resilience. This translates to creating containerized microservices that can be deployed and scaled across different environments, ensuring the applications evolve as fast as the company’s needs. By adopting this approach, you’ll accelerate development cycles, improve reliability, and pave the way for continuous innovation.

Your mission is clear: develop the core applications that will bring the AI models to life. These applications will gather video feeds from webcams, analyze worker safety in real-time, and trigger alarms when hazards are detected. 

* One application will collect and send images for inference - *Image Capture and Inference Application*

* One application will handle predictions and trigger alerts for non-compliance - *Alarm Management Service*

* One application will provide a web-based dashboard to monitor the system’s health and performance - *Dashboard and Monitoring Application*

While the inference and alarm systems will operate on the Edge Device, the dashboard will reside in the Cloud on OpenShift, bridging the factory floor with centralized oversight.

But this isn’t a solo endeavor. The project demands collaboration – you’ll work alongside data scientists, AI engineers, and operations specialists to ensure the entire solution integrates seamlessly. Development must align with broader goals like minimizing deployment times through CI/CD pipelines, ensuring the resilience of applications, and adhering to strict security protocols. Applications must be scalable, modular, and designed for longevity, reflecting the strategic goals of ACME’s software development group.

As you begin this journey, your work will not only shape the future of ACME’s safety measures but also redefine how modern applications can drive transformative change in industrial environments.

































*Create project




*create backend

apiVersion: apps/v1
kind: Deployment
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
    app.kubernetes.io/part-of: Dashboard
    app.openshift.io/runtime: "python"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: object-detection-dashboard
      component: backend
  template:
    metadata:
      labels:
        app: object-detection-dashboard
        component: backend
    spec:
      containers:
      - name: backend
        image: quay.io/luisarizmendi/object-detection-dashboard-backend:v1
        ports:
        - containerPort: 5005
---
apiVersion: v1
kind: Service
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
spec:
  selector:
    app: object-detection-dashboard
    component: backend
  ports:
  - protocol: TCP
    port: 5005
    targetPort: 5005
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
spec:
  to:
    kind: Service
    name: object-detection-dashboard-backend
  port:
    targetPort: 5005





*create fronend



apiVersion: apps/v1
kind: Deployment
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
    app.kubernetes.io/part-of: Dashboard
    app.openshift.io/runtime: "nodejs"
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"object-detection-dashboard-backend"}]'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: object-detection-dashboard
      component: frontend
  template:
    metadata:
      labels:
        app: object-detection-dashboard
        component: frontend
    spec:
      containers:
      - name: frontend
        image: quay.io/luisarizmendi/object-detection-dashboard-frontend:v1
        ports:
        - containerPort: 3000
        env:
        - name: BACKEND_API_BASE_URL
          value: http://object-detection-dashboard-backend-user99-test.apps.cluster-hkr2j.hkr2j.sandbox1307.opentlc.com
---
apiVersion: v1
kind: Service
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
spec:
  selector:
    app: object-detection-dashboard
    component: frontend
  ports:
  - protocol: TCP
    port: 3000
    targetPort: 3000
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
spec:
  to:
    kind: Service
    name: object-detection-dashboard-frontend
  port:
    targetPort: 3000










Deploy the action, inference and stream manager locally. 






podman run -it --rm --network=host -e ALERT_ENDPOINT=${DASHBOARD_BACKEND_OCP_ROUTE}/alert -e ALIVE_ENDPOINT=${DASHBOARD_BACKEND_OCP_ROUTE}/alive quay.io/luisarizmendi/object-detection-action:prod













create inference if GPUs in OpenShift





sudo podman run -it --rm -p 5000:5000 --privileged -e INFERENCE_SERVER_URL=${INFERENCE_SERVER_OCP_ROUTE} quay.io/luisarizmendi/object-detection-stream-manager:prod















