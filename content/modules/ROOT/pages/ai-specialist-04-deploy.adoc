= Model Release

The Model Release Phase is where a validated machine learning model is prepared for production use. It ensures seamless deployment, reproducibility, and scalability while maintaining accountability.

Key aspects to take into account while AI Model releasing:

* Versioning and Registry: The model is versioned and stored in a registry with metadata like training data, performance metrics, and hyperparameters. This step was already done by the Kubeflow Pipeline by publishing the model into the OpenShift AI Model Registry.
* Packaging: The model and its dependencies are prepared for consistent execution by containerizing them or converting them into standardized formats like ONNX or TorchScript. During the workshop, we demonstrated how to "export" the model from its native PyTorch format into other formats. For instance, TorchScript is particularly well-suited for edge deployments due to its compact size and efficiency. However, for simplicity in this workshop, you will continue using the native PyTorch file format. This approach ensures flexibility while allowing you to explore the practical benefits of exporting models in production scenarios.
* Validation: Additional tests, such as performance, integration, and scalability, are conducted in staging environments to ensure the model meets production requirements. In this workshop, we will perform a simple live test of the model to verify its functionality before proceeding with full deployment to edge devices.
* Deployment: Techniques like blue-green or canary deployments are commonly used to ensure a smooth and reliable transition to production. While OpenShift AI offers Model Serving capabilities through `KServe` and `ModelMesh` for models within the same cluster, deploying inference on a different environment—such as a Single Node OpenShift cluster or directly on Red Hat Enterprise Linux—requires a separate workflow to handle model transfer and deployment. In this workshop, we demonstrate the use of a custom Inference Server to host the PyTorch model, tailored for deployment beyond the primary OpenShift AI cluster.
* Monitoring: Metrics such as latency, resource usage, and data drift are continuously tracked to ensure the model's performance remains optimal. However, when deploying models at the Edge, the monitoring tools provided by OpenShift AI (e.g., TrustyAI, Grafana) cannot be used directly. Alternative monitoring strategies suited for Edge environments will be explored in detail in the next section.

In this section, we will explore how to deploy our model on a custom inference server and examine the overall application architecture, which is built on microservices. This architecture leverages the model's predictions to create a system that raises alarms when individuals are not wearing hardhats. Additionally, we will demonstrate how to conduct a quick live test of the trained model to ensure it functions as expected.


== Serving

OpenShift AI provides robust serving platforms to deploy models as inference endpoints:

* Single-model serving (`KServe`): Designed for large models requiring dedicated resources.

* Multi-model serving (`ModelMesh`): Optimized for small and medium-sized models sharing resources.

As mentioned, these options are not available for our use case since the deployment will occur at the Edge.

When deploying on Red Hat Enterprise Linux, there are several options for Model Serving. However, to simplify things in this workshop, we have created a custom Python-based inference server.



=== Overview of the solution



Before diving into the deployment details, let’s first understand the overall solution architecture, including the microservices involved and how they communicate. In this architecture we use a webcam to detect objects at the edge, and how those detections can trigger messages/alarms that can be visualized in a dashboard on the Core Datacenter/Cloud.


image::ai-deploy-object-detection-webcam.png[]

The solution is based on the following microservices, you can clik on the names to get detailed information about each one.

* https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-stream-manager/README.md[Camera Stream Manager]

* https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-inference-server/README.md[Inference server]

* https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-action/README.md[Actuator service]

* https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-dashboard/src/backend/README.md[Dashboard backend]

* https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/serving/apps/object-detection-dashboard/src/frontend/README.md[Dashboard frontend]


The workflow is the following:

1. The Camera Stream Manager sends images to the Inference API
2. The Inference Server, that contains the AI model detecting objects, returns the predictions
3. The "action" service calls the inference endpoint and if detects certain objects it will trigger an alarm, that is sent to the database hosted in a remote site.
4. The information of the database is shown in the database



Although application development is not covered in this workshop, it plays a fundamental role and can also be carried out on OpenShift. OpenShift provides several tools to support application development, such as:

* *OpenShift Developer Hub*: This internal developer portal centralizes access to resources, templates, and documentation, accelerating the onboarding process and standardizing application development across teams. It ensures consistency and visibility into microservices and APIs.

* *OpenShift Dev Spaces*: OpenShift Dev Spaces provides cloud-based development environments accessible directly from the browser. It offers pre-configured, containerized workspaces that mirror production, ensuring developers can write, test, and debug code in an environment that reflects the final deployment conditions.

* *OpenShift Pipelines*: Built on Tekton, OpenShift Pipelines automates CI/CD workflows, enabling fast, consistent builds, tests, and deployments of containerized applications. This tool ensures rapid iteration and integration, reducing downtime and accelerating feature delivery.

* *Quay Container Image Registry*: A secure container image registry stores and manages application images, ensuring that developers can reliably push, pull, and deploy containers to different environments. It supports versioning and helps enforce security and compliance policies.

* *OpenShift GitOps*: Implements GitOps practices for application deployment and lifecycle management.


All of these tools can be utilized throughout the application development lifecycle, which mirrors the MLOps cycle we are following in this workshop. Both cycles share common stages, such as:
image::dev-workflow.png[]

1. *App Planning*: Architecture Design: This foundational phase focuses on system design decisions, technology stack selection, and establishing the technical approach. It sets the blueprint for the entire application development lifecycle.

2. *App Development*: This encompasses the core development activities:

    * Code Development: Writing application code following established design patterns and best practices. This involves implementing features and functionality according to requirements.
    * Testing: Comprehensive testing. This phase often requires iteration back to code development to address identified issues.

3. *App Release*: Integration, Deployment: After successful testing, the application is prepared for production, involving integration with other systems and services, deployment through CI/CD pipelines,final verification in staging environments and production rollout

4. *Day-2 Operations*: Monitoring, Tuning: Post-deployment activities focus on application performance monitoring, resource utilization optimization and performance tuning based on real world usage



























multiarch 











app info 


appdev













=== Custom Inference Server

















== Live Testing






image::ai-deploy-screenshot_video_stream.png[]








image::ai-deploy-screenshot_dashboard_main.png[]





image::ai-deploy-screenshot_dashboard_detail.png[]













Find below the workflow that you will follow as Application Developer during the workshop (each of those will be a different "module").




This represents a continuous cycle rather than a linear process, with several feedback loops driving ongoing improvement:

    * From operations back to architecture design: Performance metrics and operational insights inform architectural decisions and improvements
    * From operations to development: Production issues may require code changes or optimizations
    * From testing back to development: Issues found during testing lead to code refinement
    * From deployment experiences back to architecture: Real-world deployment challenges might necessitate architectural adjustments

The feedback loops ensure continuous improvement and adaptation to changing requirements, performance needs, and operational realities. For successful application development, it's crucial to understand that you may need to revisit earlier phases as new information or challenges emerge.

Now that you understand your requirements, your task and the workflow that you should follow, you can jump directly into the first module: xref:app-developer-01-arch.adoc[App Planning].





































*Create project




*create backend

apiVersion: apps/v1
kind: Deployment
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
    app.kubernetes.io/part-of: Dashboard
    app.openshift.io/runtime: "python"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: object-detection-dashboard
      component: backend
  template:
    metadata:
      labels:
        app: object-detection-dashboard
        component: backend
    spec:
      containers:
      - name: backend
        image: quay.io/luisarizmendi/object-detection-dashboard-backend:v1
        ports:
        - containerPort: 5005
---
apiVersion: v1
kind: Service
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
spec:
  selector:
    app: object-detection-dashboard
    component: backend
  ports:
  - protocol: TCP
    port: 5005
    targetPort: 5005
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
spec:
  to:
    kind: Service
    name: object-detection-dashboard-backend
  port:
    targetPort: 5005





*create fronend



apiVersion: apps/v1
kind: Deployment
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
    app.kubernetes.io/part-of: Dashboard
    app.openshift.io/runtime: "nodejs"
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"object-detection-dashboard-backend"}]'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: object-detection-dashboard
      component: frontend
  template:
    metadata:
      labels:
        app: object-detection-dashboard
        component: frontend
    spec:
      containers:
      - name: frontend
        image: quay.io/luisarizmendi/object-detection-dashboard-frontend:v1
        ports:
        - containerPort: 3000
        env:
        - name: BACKEND_API_BASE_URL
          value: http://object-detection-dashboard-backend-user99-test.apps.cluster-hkr2j.hkr2j.sandbox1307.opentlc.com
---
apiVersion: v1
kind: Service
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
spec:
  selector:
    app: object-detection-dashboard
    component: frontend
  ports:
  - protocol: TCP
    port: 3000
    targetPort: 3000
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
spec:
  to:
    kind: Service
    name: object-detection-dashboard-frontend
  port:
    targetPort: 3000










Deploy the action, inference and stream manager locally. 






podman run -it --rm --network=host -e ALERT_ENDPOINT=${DASHBOARD_BACKEND_OCP_ROUTE}/alert -e ALIVE_ENDPOINT=${DASHBOARD_BACKEND_OCP_ROUTE}/alive quay.io/luisarizmendi/object-detection-action:prod













create inference if GPUs in OpenShift





sudo podman run -it --rm -p 5000:5000 --privileged -e INFERENCE_SERVER_URL=${INFERENCE_SERVER_OCP_ROUTE} quay.io/luisarizmendi/object-detection-stream-manager:prod















