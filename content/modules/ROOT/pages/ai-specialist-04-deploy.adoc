




















== Workflow Overview

Find below the workflow that you will follow as Application Developer during the workshop (each of those will be a different "module").

image::dev-workflow.png[]

1. *App Planning*: Architecture Design: This foundational phase focuses on system design decisions, technology stack selection, and establishing the technical approach. It sets the blueprint for the entire application development lifecycle.

2. *App Development*: This encompasses the core development activities:

    * Code Development: Writing application code following established design patterns and best practices. This involves implementing features and functionality according to requirements.
    * Testing: Comprehensive testing. This phase often requires iteration back to code development to address identified issues.

3. *App Release*: Integration, Deployment: After successful testing, the application is prepared for production, involving integration with other systems and services, deployment through CI/CD pipelines,final verification in staging environments and production rollout

4. *Day-2 Operations*: Monitoring, Tuning: Post-deployment activities focus on application performance monitoring, resource utilization optimization and performance tuning based on real world usage

This represents a continuous cycle rather than a linear process, with several feedback loops driving ongoing improvement:

    * From operations back to architecture design: Performance metrics and operational insights inform architectural decisions and improvements
    * From operations to development: Production issues may require code changes or optimizations
    * From testing back to development: Issues found during testing lead to code refinement
    * From deployment experiences back to architecture: Real-world deployment challenges might necessitate architectural adjustments

The feedback loops ensure continuous improvement and adaptation to changing requirements, performance needs, and operational realities. For successful application development, it's crucial to understand that you may need to revisit earlier phases as new information or challenges emerge.

Now that you understand your requirements, your task and the workflow that you should follow, you can jump directly into the first module: xref:app-developer-01-arch.adoc[App Planning].













You have been chosen as the lead application developer for a critical safety initiative at ACME Your role is pivotal in building a suite of modern applications that will integrate cutting edge AI models developed by your colleagues to prevent accidents on the factory floor. As you step into this role, the challenge ahead is not just about coding, it’s about designing and deploying software that will safeguard lives and transform workplace safety.

ACME is embracing a "modern application development" approach, signaling a shift towards technologies and architectures that enable greater scalability, flexibility, and resilience. This translates to creating containerized microservices that can be deployed and scaled across different environments, ensuring the applications evolve as fast as the company’s needs. By adopting this approach, you’ll accelerate development cycles, improve reliability, and pave the way for continuous innovation.

Your mission is clear: develop the core applications that will bring the AI models to life. These applications will gather video feeds from webcams, analyze worker safety in real-time, and trigger alarms when hazards are detected. 

* One application will collect and send images for inference - *Image Capture and Inference Application*

* One application will handle predictions and trigger alerts for non-compliance - *Alarm Management Service*

* One application will provide a web-based dashboard to monitor the system’s health and performance - *Dashboard and Monitoring Application*

While the inference and alarm systems will operate on the Edge Device, the dashboard will reside in the Cloud on OpenShift, bridging the factory floor with centralized oversight.

But this isn’t a solo endeavor. The project demands collaboration – you’ll work alongside data scientists, AI engineers, and operations specialists to ensure the entire solution integrates seamlessly. Development must align with broader goals like minimizing deployment times through CI/CD pipelines, ensuring the resilience of applications, and adhering to strict security protocols. Applications must be scalable, modular, and designed for longevity, reflecting the strategic goals of ACME’s software development group.

As you begin this journey, your work will not only shape the future of ACME’s safety measures but also redefine how modern applications can drive transformative change in industrial environments.

































*Create project




*create backend

apiVersion: apps/v1
kind: Deployment
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
    app.kubernetes.io/part-of: Dashboard
    app.openshift.io/runtime: "python"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: object-detection-dashboard
      component: backend
  template:
    metadata:
      labels:
        app: object-detection-dashboard
        component: backend
    spec:
      containers:
      - name: backend
        image: quay.io/luisarizmendi/object-detection-dashboard-backend:v1
        ports:
        - containerPort: 5005
---
apiVersion: v1
kind: Service
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
spec:
  selector:
    app: object-detection-dashboard
    component: backend
  ports:
  - protocol: TCP
    port: 5005
    targetPort: 5005
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: object-detection-dashboard-backend
  labels:
    app: object-detection-dashboard
spec:
  to:
    kind: Service
    name: object-detection-dashboard-backend
  port:
    targetPort: 5005





*create fronend



apiVersion: apps/v1
kind: Deployment
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
    app.kubernetes.io/part-of: Dashboard
    app.openshift.io/runtime: "nodejs"
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"object-detection-dashboard-backend"}]'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: object-detection-dashboard
      component: frontend
  template:
    metadata:
      labels:
        app: object-detection-dashboard
        component: frontend
    spec:
      containers:
      - name: frontend
        image: quay.io/luisarizmendi/object-detection-dashboard-frontend:v1
        ports:
        - containerPort: 3000
        env:
        - name: BACKEND_API_BASE_URL
          value: http://object-detection-dashboard-backend-user99-test.apps.cluster-hkr2j.hkr2j.sandbox1307.opentlc.com
---
apiVersion: v1
kind: Service
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
spec:
  selector:
    app: object-detection-dashboard
    component: frontend
  ports:
  - protocol: TCP
    port: 3000
    targetPort: 3000
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: object-detection-dashboard-frontend
  labels:
    app: object-detection-dashboard
spec:
  to:
    kind: Service
    name: object-detection-dashboard-frontend
  port:
    targetPort: 3000










Deploy the action, inference and stream manager locally. 






podman run -it --rm --network=host -e ALERT_ENDPOINT=${DASHBOARD_BACKEND_OCP_ROUTE}/alert -e ALIVE_ENDPOINT=${DASHBOARD_BACKEND_OCP_ROUTE}/alive quay.io/luisarizmendi/object-detection-action:prod













create inference if GPUs in OpenShift





sudo podman run -it --rm -p 5000:5000 --privileged -e INFERENCE_SERVER_URL=${INFERENCE_SERVER_OCP_ROUTE} quay.io/luisarizmendi/object-detection-stream-manager:prod






















https://github.com/opendatahub-io/ai-edge












2. Model Serving

Overview

OpenShift AI provides robust serving platforms to deploy models as inference endpoints:

Single-model serving (KServe): Designed for large models requiring dedicated resources.

Multi-model serving (ModelMesh): Optimized for small and medium-sized models sharing resources.

Usage with RHEL Inference

Containerize models using OpenShift AI’s serving runtime (e.g., NVIDIA Triton, TensorFlow Serving).

Push containerized models to a container registry for deployment on RHEL.

Use Podman or Docker on RHEL to deploy and serve the model locally.

3. Monitoring with TrustyAI

Overview

TrustyAI in OpenShift AI enables monitoring of machine learning models for fairness, bias, and drift. It integrates with OpenShift’s monitoring stack to provide insights into model behavior and performance.

Usage with RHEL Inference

Metrics Collection:

Use Prometheus and Node Exporter on RHEL to collect system and inference metrics.

Forward these metrics to OpenShift’s Prometheus instance.

Bias and Drift Analysis:

Send inference inputs and outputs from RHEL to OpenShift AI for analysis using TrustyAI.

Leverage TrustyAI’s bias and drift metrics for ongoing model evaluation.










ai-deploy-serving-type.png

->  Multi-model serving platform









https://docs.openvino.ai/2024/index.html







https://ai-on-openshift.io/demos/yolov5-training-serving/yolov5-training-serving/#consuming-the-model-over-grpc




test in openshift 



https://github.com/rh-aiservices-bu/fraud-detection






https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html-single/serving_models/index#supported-model-serving-runtimes_serving-large-models






Best Options for YOLO Model Serving

    vLLM ServingRuntime for KServe:
        Why? It supports PyTorch (.pt) and ONNX models and is optimized for single-model serving via REST.
        Use this runtime for deploying YOLO models effectively, especially if you're using PyTorch models.

    OpenVINO Model Server (Multi-Model):
        Why? Ideal for ONNX models and optimized for inference on Intel hardware.
        If you have exported the YOLO model to .onnx, this is an excellent choice for high-performance serving.

    Caikit Standalone ServingRuntime for KServe:
        Why? Suitable for single PyTorch models (.pt) served over REST.
        If you want to stick with the native .pt format without conversion, this is a straightforward option.













a "kind: InferenceService" object is created when deployed






















https://github.com/opendatahub-io/caikit















































curl https://hardhat-user99-ai.apps.cluster-r2h4p.r2h4p.sandbox3268.opentlc.com/v2
{"name":"OpenVINO Model Server","version":"2024.3.0"}




(change model name to the one that you put in when deployed)
curl https://hardhat-user99-ai.apps.cluster-r2h4p.r2h4p.sandbox3268.opentlc.com/v2/models/hardhat/versions/1/ready






curl https://hardhat-user99-ai.apps.cluster-r2h4p.r2h4p.sandbox3268.opentlc.com/v2/models/hardhat
{"name":"hardhat","versions":["1"],"platform":"OpenVINO","inputs":[{"name":"images","datatype":"FP32","shape":[1,3,640,640]}],"outputs":[{"name":"output0","datatype":"FP32","shape":[1,6,8400]}]}









https://docs.openvino.ai/2024/openvino-workflow/model-server/ovms_docs_rest_api_kfs.html#inference-api



https://hardhat-user99-ai.apps.cluster-r2h4p.r2h4p.sandbox3268.opentlc.com/v2/models/hardhat/versions/1/infer





https://blog.openvino.ai/blog-posts/kserve-api






app para probar: 

CLI - explicar


UI

podman build -t quay.io/luisarizmendi/object-detection-batch-model-api:latest .

podman run -it --rm -p 8800:8800 quay.io/luisarizmendi/object-detection-batch-model-api:latest












https://developer.hpe.com/blog/production-ready-object-detection-model-training-workflow-with-hpe-machine-learning-development-environment/