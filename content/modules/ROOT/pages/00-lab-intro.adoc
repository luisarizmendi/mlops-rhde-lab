= Lab Introduction

The workshop requires several components, but there are multiple architectures to consider for assembling these components effectively.

Let's outline the necessary components and then explore the architecture used for this workshop.

== Workshop Building Blocks

Local (Edge) Components:

* Edge Devices: Devices where the application and AI model will run. It is recommended that these devices have a GPU to accelerate AI inference.

* Webcams: Since this workshop focuses on image recognition, connecting a webcam to edge devices is essential.

* Workstation/Laptop: This is your main system for following the workshop instructions.

* Network Infrastructure: Switches and routers (with additional services such as DNS and DHCP) to ensure communication between all components. Internet connectivity may be required depending on the final lab architecture.

Additional Services:

* Source Code Repository: Hosts the workshop guide, application, AI model code, infrastructure descriptors, and scripts.

* Container Image Registry: Containerized workloads require a registry to store container images.

* Edge Device Image Builder: Building image-based operating systems simplifies edge computing deployment and enhances consistency. A dedicated image-building system is necessary.

* AI Model Development Environment: A platform to train and refine AI models.

* APP Development Environment: A system to develop and build containerized applications.

* Edge Device Manager: A system used to manage the Edge Devices.

Other Essentials:

* Props: Since this workshop focuses on object detection, you need physical objects to test AI inferencing.

You can run the entire lab locally. This requires not only edge devices, webcams, and a laptop but also an additional server to manage services such as the container registry, source code repository, image builder, and development environments. This could be a standalone server running RHEL or one or multiple systems running an OpenShift cluster.

Deploying OpenShift can enhance the lab by providing tools like OpenShift AI for AI model training and OpenShift Pipelines for application development. If you opt to train AI models from scratch (instead of using pre-trained models), ensure you have GPU resources.

Alternatively to the local lab, some or all additional services can be hosted in the cloud to reduce local hardware requirements.

== Lab Architecture Overview

For this lab, the following architecture will be used:

image::labintro-arch.png[]

Running Locally:

* Edge Devices: Jetson Orin NANO Development Board (8GB RAM) powered by NVIDIA for AI deep learning.

* Webcams: Arducam 16MP Autofocus USB Camera.

* Network Infrastructure: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

* Local Server: OnLogic Helix 500 with XXXXXXXXXX cores, XXXXXXXXXXXX GB memory, and XXXXXXXXXXX disk capacity. The local server will run the following services:
    - Container Image Registry mirror (to save bandwidth).

[NOTE]

Automatic mirroring is not configured. If you build new container images using pipelines, push them manually to the local registry. Alternatively, configure pipelines to use external registries, though expect longer wait times during image pulls.



Running in the Cloud:

* Source Code Repository: GitHub.

* Container Image Registry: Quay.io.

* AI Model Development Environment: OpenShift AI.

* APP Development Environment: OpenShift Dev Spaces + OpenShift Pipelines.

* Edge Device Image Builder: ???????????????????????????????????????????????????????????????????????????

* Edge Device Manager: xref:https://github.com/flightctl/flightctl[Flight Control] running on top of OpenShift.

[NOTE]

OpenShift services will run on AWS.


Additional Items:

* Props: Hardhats and hats for object detection.


For details on how the lab environment was deployed, refer to the xref:00-how_to_deploy_lab.adoc[Deployment Guide].


== Workstation/Laptop Requirements

The only requirements are:

* SSH Client – to connect to remote services.

* Web Browser – to access workshop materials and cloud platforms.


== Getting Started

Before beginning the workshop steps, you will need some necessary environment details (e.g., URLs, usernames, passwords, etc). Some of this information is pre-configured as global variables in the xref:https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/content/antora.yml[antora.yml] file. These variables will automatically populate in the workshop guide.

However, there is user-specific information that must be entered manually. Your instructor will provide these values. Use the input boxes in the guide’s header to fill them in. If not entered, placeholders (e.g., USER_NAME) will appear in the guide.

Now you're ready to choose your path in the navigation menu and let's get started!

