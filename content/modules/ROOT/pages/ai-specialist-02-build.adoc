= Model Building

The process of AI model building is a multistep journey that transforms raw data into a functional model. It encompasses several key stages, each playing a crucial role in ensuring the model can learn, generalize, and ultimately perform as required. This section will guide you through the essential steps of engineering, training, and evaluation.

While these three stages (Engineering, Training, and Evaluation) are commonly used to describe the model-building process, it's important to note that there are other ways to segment the workflow. For example, some frameworks emphasize Feature Engineering as distinct stages, while others may break down Model Selection or Hyperparameter Optimization as separate steps. Each approach can vary depending on the complexity of the project and the specific needs of the model being developed.


=== Tools and preparations

In this section we will be using the following tools: 

* Source Code Repository: We will use GitHub or Gitea to store and version-control the Jupyter Notebooks developed for preliminary model training. Additionally, it will house the automated pipelines that streamline and manage the training process as the project evolves.

* Object Storage: Object storage provides a scalable and accessible solution for persisting large files. Once our model is trained, we will leverage object storage to securely store and manage the model artifacts, ensuring they remain accessible for deployment and further iterations.

* OpenShift AI: OpenShift AI is an integrated platform that simplifies the development and deployment of AI workloads. We will use it to create and manage the Jupyter Notebooks for experimentation and model training. Furthermore, it will enable the creation and orchestration of automated pipelines, ensuring efficient and repeatable workflows.

In this workshop we will explore  the features of OpenShift AI and their applicability when creating models in OpenShift AI while performing inference on RHEL. OpenShift AI is designed to deploy and serve models directly on OpenShift, leveraging its built-in capabilities for scalability, monitoring, and orchestration. However, it is also possible to leverage OpenShift AI features for workflows where inference is performed on RHEL.

When adopting this hybrid approach, you need to bear in mind the following:

* Model Compatibility: Ensure the model format is supported by the serving runtime you plan to use on RHEL.

* Artifact Retrieval: Models stored in the OpenShift AI Model Registry must be exported and transferred to the RHEL environment.

* Monitoring Integration: Set up a feedback loop to forward inference metrics and logs from RHEL to OpenShift AI for analysis is not a built-in feature when performing inference in RHEL.

* Security Considerations: Implement secure communication between RHEL and OpenShift AI.

Some features of OpenShift AI will be easier to use when performing inference in RHEL, as they do not require adaptation. For example, using Jupyter Notebooks for model training and export is the same process regardless of the deployment target. However, features such as monitoring or serving will require adjustments to accommodate their use in a RHEL environment, such as setting up Prometheus endpoints or deploying containerized models locally.

In this section Model Building we will be using OpenShift AI capabilities with some adaptations detailed in the respective points below.


[NOTE]

In the next section xref:ai-specialist-03-deploy.adoc[Model Release] we will explore additional OpenShift AI features that are useful when preparing your model to be used in RHEL systems (in contrast when you perform the inference in OpenShift)

* Data Science Projects and Jupyter Notebooks: OpenShift AI organizes machine learning workflows into projects, providing a collaborative environment for data scientists. Projects integrate Jupyter Notebooks for data preprocessing, model experimentation, and training. It provides a scalable environment with access to GPUs and shared storage.

No real addaptation is needed when inference in performed in RHEL, you need to develop and validate models in Jupyter Notebooks and then export the trained model artifacts to the Model Registry or directly to RHEL (explained in xref:ai-specialist-03-deploy.adoc[Model Release] section).

* OpenShift Pipelines: OpenShift Pipelines is a CI/CD solution for automating workflows, including model training, serving, and monitoring. In the building context, this feature will be used to automate the retrieval, training and containerization of then model. Pipelines are also useful to help with model serving (when deploying on OpenShift) and to periodically update models based on feedback or retraining needs (explained in xref:ai-specialist-04-update.adoc[Day-2 Operations] section).


[NOTE]

There are other interesting features such as Distributed Training that are not covered in this workshop.










The OpenShift AI platform is already preconfigured (OpenShift AI infrastructure configuration is out of the scope of this workshop)








preparation
----------------------------




 since we will be using a source code repository to store the notebook and ensure version control, you first need to create a new repository.

[example]
====
If you plan to use Gitea (or any other accessible source code repository such as GitHub), you can create a new repository by following these steps:

1. Navegate to {gitea}
2. Register (top right button)
3. Click + in "Repositories" to create a new repository
4. Fill-in repository data (you might want to use `{openshift-user-basename}pass:[<span id="gnumberVal"></span>]-ai-prototyping` as name)

====






Create object storage buckets









== Engineering


Engineering is the foundation of the model building process, where data and features are prepared and transformed into a form that can be consumed by the model. This stage involves selecting appropriate algorithms and designing architectures.

Selecting appropriate algorithms involves analyzing the problem type, such as classification or regression, and understanding the data's characteristics to identify the best-fit solution. This process requires balancing performance metrics like accuracy, interpretability, and computational efficiency through experimentation. Designing architectures focuses on defining the structure of the model by choosing the right combination of layers, activation functions, and hyperparameters to capture the complexity of the data. 

We already explained that our project involves a classification problem, and that the pre-selected algorithm is YOLO on its version 11. YOLOv11 (You Only Look Once, version 11) is the latest evolution in the https://docs.ultralytics.com/es/models/[YOLO family] of object detection models, building on its predecessors to achieve faster and more accurate results. This model is designed to meet the growing demands of near real-time object detection applications in fields such as autonomous vehicles, video surveillance, robotics, and more.

At this stage, it is essential to focus on choosing the right parameters during training, such as the learning rate, batch size, input image size, number of epochs, optimizer, etc. These parameters significantly impact the model performance, and fine tuning them is critical for achieving optimal results. Prototyping plays a crucial role in this process, allowing you to experiment with various configurations and refine model architectures iteratively. A common and effective way to perform this experimentation is by using https://jupyter.org/[Jupyter Notebooks].

Jupyter Notebooks are an interactive computing environment that combines live code, visualizations, and narrative text in a single document. They are ideal for prototyping machine learning models because they allow you to quickly test, debug, and document your workflows in a user-friendly interface.

[example]
====
To get started, you will create a new, empty Jupyter Notebook using OpenShift AI. In order to do so you have to 

1. Navegate to {openshift-ai}  
2. Create a new Data Science Project ({openshift-user-basename}pass:[<span id="gnumberVal"></span>]-ai-prototyping)
3. Create a new Storage Connection
4. Create a new Workbench
5. Once started, open the Workbench (it could take time the first time)
====

[NOTE]

Log in using your OpenShift credentials: {openshift-user-basename}pass:[<span id="gnumberVal"></span>]  /  {openshift-password}





ai-build-dataconnection.png  -> vover a cogerla sin el http


ai-build-workbench.png


ai-build-gitclone.png




1. Clone the source code repository
2. Create a `{openshift-user-basename}pass:[<span id="gnumberVal"></span>]-ai-prototyping.ipynb` file on the cloned directory









tip:
rebalance train split to reduce to num images = batch size, create new version and use it 







ai-build-protomodels.png







kernel crash when playing iwth batch sizec





EACH EPOCH LAST xxxxx






explain knowledge transfer



explain jupyter notebook





























== Training

Training is the next phase, where the prepared data is fed into the model, and the model's parameters are iteratively adjusted to minimize errors and improve performance. This step requires careful attention to hyperparameter tuning, managing computational resources, and monitoring the training process to avoid issues like overfitting or underfitting.








Preparing the training scripts involves creating a pipeline for data preprocessing, defining the loss function and optimization algorithm, writing training loops to iteratively improve the model, and incorporating logging tools like TensorBoard to monitor and fine-tune progress while ensuring robustness through error handling mechanisms.






explain variables notebook 



run 

























https://ai-on-openshift.io/tools-and-applications/minio/minio/#uninstall-instructions






== Evaluation

Evaluation is crucial in determining how well the model has learned from the training data and how effectively it can generalize to unseen data. Metrics such as accuracy, precision, recall, and F1 score are used to assess the model's performance. Rigorous evaluation helps identify potential areas for improvement and ensures that the model meets the desired standards before deployment.



explain evaluation methods

explain graphs




blah, blah




== Solution and Next Steps
