= Model Building

The process of AI model building is a multistep journey that transforms raw data into a functional model. It encompasses several key stages, each playing a crucial role in ensuring the model can learn, generalize, and ultimately perform as required. This section will guide you through the essential steps of engineering, training, and evaluation.

While these two stages (Feature Engineering and Model development) are commonly used to describe the model-building process, it's important to note that there are other ways to segment the workflow. For example, some frameworks emphasize Feature Engineering as distinct stages, while others may break down Model Selection or Hyperparameter Optimization as separate steps. Each approach can vary depending on the complexity of the project and the specific needs of the model being developed.


=== Tools and preparations

In this section we will be using the following tools: 

*OpenShift AI*

OpenShift AI is an integrated platform that simplifies the development and deployment of AI workloads. We will use it to create and manage the Jupyter Notebooks for experimentation and model training. Furthermore, it will enable the creation and orchestration of automated pipelines, ensuring efficient and repeatable workflows.

In this workshop we will explore  the features of OpenShift AI and their applicability when creating models in OpenShift AI while performing inference on RHEL. OpenShift AI is designed to deploy and serve models directly on OpenShift, leveraging its built-in capabilities for scalability, monitoring, and orchestration. However, it is also possible to leverage OpenShift AI features for workflows where inference is performed on RHEL.

When adopting this hybrid approach, you need to bear in mind the following:

* Model Compatibility: Ensure the model format is supported by the serving runtime you plan to use on RHEL.

* Artifact Retrieval: Models stored in the OpenShift AI Model Registry must be exported and transferred to the RHEL environment.

* Monitoring Integration: Set up a feedback loop to forward inference metrics and logs from RHEL to OpenShift AI for analysis is not a built-in feature when performing inference in RHEL.

* Security Considerations: Implement secure communication between RHEL and OpenShift AI.

Some features of OpenShift AI will be easier to use when performing inference in RHEL, as they do not require adaptation. For example, using Jupyter Notebooks for model training and export is the same process regardless of the deployment target. However, features such as monitoring or serving will require adjustments to accommodate their use in a RHEL environment, such as setting up Prometheus endpoints or deploying containerized models locally.

In this section Model Building we will be using Data Science Projects and Jupyter Notebooks. OpenShift AI organizes machine learning workflows into projects, providing a collaborative environment for data scientists. Projects integrate Jupyter Notebooks for data preprocessing, model experimentation, and training. It provides a scalable environment with access to GPUs and shared storage.

No real addaptation for these componentes are needed when inference in performed in RHEL, you need to develop and validate models in Jupyter Notebooks and then export the trained model artifacts to the Model Registry or directly to RHEL (explained in xref:ai-specialist-03-deploy.adoc[Model Release] section).

[NOTE]

In the next section xref:ai-specialist-03-deploy.adoc[Model Release] we will explore additional OpenShift AI features that are useful when preparing your model to be used in RHEL systems (in contrast when you perform the inference in OpenShift)

[NOTE]

There are other interesting features such as Distributed Training that are not covered in this workshop.


[example]
====

NVIDIA USAGE IN OPENSHIFT AI
====






*Source Code Repository (Gitea)*

We will use Gitea to store and version-control the Jupyter Notebooks developed for preliminary model training, but any other source code repository such as GitHub could be used. Additionally, it will house the automated pipelines that streamline and manage the training process as the project evolves.

[example]
====
If you plan to use Gitea you can create a new repository by following these steps:

1. Navegate to {gitea}
2. Click "Register" (top right button) and fill in required fields (use "userpass:[<span id="gnumberVal"></span>]" as username)
3. After registering, click `+` in "Repositories" to create a new repository
4. Fill in repository data (use "userpass:[<span id="gnumberVal"></span>]-ai" as repository name)

====



*Object Storage (MinIO)*

Object storage provides a scalable and accessible solution for persisting large files. Once our model is trained, we will leverage object storage to securely store and manage the model artifacts, ensuring they remain accessible for deployment and further iterations.

In this workshop MinIO, an Open Source High Performance Object Storage, is deployed in the environment, and user "{minio-user-base}pass:[<span id="gnumberVal"></span>]" has been created for you with password "{minio-password-base}pass:[<span id="gnumberVal"></span>]".

[example]
====
To store the model that you will create during this prototyping phase, you need to create a new Object Bucket

1. Navegate to {minio-ui}
2. Log in with {minio-user-base}pass:[<span id="gnumberVal"></span>] / {minio-password-base}pass:[<span id="gnumberVal"></span>]
3. Go to "Buckets" in the left menu and then click on "Create Bucket" on the top right corner
4. Create a new Bucket with the name "userpass:[<span id="gnumberVal"></span>]-ai"
====

[CAUTION]

The MinIO users provided for this workshop have administrative privileges, allowing you to fully explore and experiment with the platform without any restrictions. However, please exercise caution to avoid inadvertently modifying other users' buckets or altering user privileges.




== Feature Engineering


Feature Engineering is the foundation of the model building process, where data and features are prepared and transformed into a form that can be consumed by the model. This stage involves selecting appropriate algorithms and designing architectures.

Selecting appropriate algorithms involves analyzing the problem type, such as classification or regression, and understanding the data's characteristics to identify the best-fit solution. This process requires balancing performance metrics like accuracy, interpretability, and computational efficiency through experimentation. Designing architectures focuses on defining the structure of the model by choosing the right combination of layers, activation functions, and hyperparameters to capture the complexity of the data. 

We already explained that our project involves a classification problem, and that the pre-selected algorithm is YOLO on its version 11. YOLOv11 (You Only Look Once, version 11) is the latest evolution in the https://docs.ultralytics.com/es/models/[YOLO family] of object detection models, building on its predecessors to achieve faster and more accurate results. This model is designed to meet the growing demands of near real-time object detection applications in fields such as autonomous vehicles, video surveillance, robotics, and more.


During the dataset preparation phase, we incorporated essential feature engineering steps, including balancing the dataset, ensuring high-quality annotations, applying augmentations like flipping, scaling, and mosaic augmentation, and normalizing image sizes. These steps laid a robust foundation for our data.

However, these aspects can also be revisited and refined during feature engineering to further enhance model performance. Additionally, during model development, we can introduce advanced augmentation and preprocessing techniques not included in the data management phase, such as domain-specific augmentations, fine-tuned normalization strategies, or even dynamically generated transformations, tailored to YOLOv11's capabilities.

This step requires no actions in this workshop, as these aspects were reviewed and addressed during the Data Management phase, just bear in mind that you cound need to revisit them as part of the Model Development cycle to obtain a better performance.


== Model Development

At this stage, it is essential to focus on choosing the right hyperparameters during training, such as the learning rate, batch size, input image size, number of epochs, optimizer, etc. These parameters significantly impact the model performance, and fine tuning them is critical for achieving optimal results. Prototyping plays a crucial role in this process, allowing you to experiment with various configurations and refine model architectures iteratively. A common and effective way to perform this experimentation is by using https://jupyter.org/[Jupyter Notebooks].

Jupyter Notebooks are an interactive computing environment that combines live code, visualizations, and narrative text in a single document. They are ideal for prototyping machine learning models because they allow you to quickly test, debug, and document your workflows in a user-friendly interface.

[example]
====
To get started, you will create a new, empty Jupyter Notebook using OpenShift AI. In order to do so you have to 

1- Navegate to {openshift-ai}. Log in using your OpenShift credentials: {openshift-user-base}pass:[<span id="gnumberVal"></span>]  /  {openshift-password} 

2- Create a new Data Science Project "userpass:[<span id="gnumberVal"></span>]-ai"

3- Create a new Storage Connection using your MinIO username and password ( {minio-user-base}pass:[<span id="gnumberVal"></span>] / {minio-password-base}pass:[<span id="gnumberVal"></span>] ), the MinIO API URL ({minio-api}) without the `http://`, and the Bucket that you created ("userpass:[<span id="gnumberVal"></span>]-ai"). 

image::ai-build-dataconnection.png[]

4- Create a new Workbench named "Jupyter-prototyping". You will need to select the base image that will be used to run your Jupyter Notebooks (select `PyTorch`), the Container Size (`Small` is enough), the Persistent Volume associated to the container (you can keep the default 20Gi Persistent Volume for your Notebook if you don't plan to run a lot of training runs) and configure the Object Storage Connection that you already configured.

image::ai-build-workbench.png[]

5- Once started, open the Workbench (it could take time the first time)

6- Clone the source code repository that you created ("userpass:[<span id="gnumberVal"></span>]-ai") using the left menu.

image::ai-build-gitclone.png[]

7- Create a `userpass:[<span id="gnumberVal"></span>]-ai.ipynb` file inside the cloned directory ("userpass:[<span id="gnumberVal"></span>]-ai")
====

It's time to begin working on the Jupyter Notebook you just created. Below, you will find subsections that explain each necessary code block. To get started, create new code blocks by clicking the `+` button in the top menu. Configure each block based on the instructions provided, then run the block by clicking the play button to ensure it works as expected. You are encouraged to add additional Markdown cells for further explanations or adjust the provided code to suit your needs. This hands-on approach will help you gain a deeper understanding and tailor the notebook to your specific project.

Let's start with the first code block, the dependencies.

[TIP]

If you'd prefer to skip the process of configuring each code block or simply want to see the completed version, the https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/resources/solutions/ai-specialist/prototyping.ipynb[full Jupyter Notebook is available for you to review here]. This allows you to quickly access the final file without spending time on the setup.


=== Dependencies

Here’s an enhanced version of your text:

When setting up the Workbench to run your Jupyter Notebook, you were required to select one of the available base container images (e.g., `Pytorch`). The Jupyter Notebook will execute within this environment, which means all the pre-installed packages and tools in that container image will be readily available.

In our case, however, we will need additional packages, such as the one that allows accessing the dataset directly from Roboflow. These packages may not be included in the selected base image, so it’s essential to install them manually. You can do this by running the following `pip install` command:

[source,python,role=execute,subs="attributes"]
----
# For Training
!pip install ultralytics roboflow 

# For Storage
!pip install minio
----

[IMPORTANT]

Once you have identified all the required packages, consider creating a custom base image that includes these dependencies. This optimized image will streamline not only the prototyping phase but also regular training workflows performed through Pipelines.

=== Python Libraries

Import all necessary libraries for training and analysis. Basically you will need:

* Libraries for training: 

This block will be dependant on your Python code, but probably you will need the following imports:

[source,python,role=execute,subs="attributes"]
----
# Common
import  os

# For Dataset manipulation
import yaml
from roboflow import Roboflow

# For training
import torch
from ultralytics import YOLO

# For Storage
from minio import Minio
from minio.error import S3Error
----


=== Roboflow Dataset download

The next step is to download the dataset prepared in the xref:ai-specialist-01-data.adoc[Data Management] section. Instead of manually downloading the ZIP file, we will access the dataset directly from Roboflow for a more streamlined process. When you created the "Roboflow Version" of the dataset, you received a unique code to access it. Now, it's time to put that code to use.

Double check that you're using the correct API Key, Workspace name, Project name, and Version number to ensure a seamless connection to the dataset.

[CAUTION]
If you have multiple versions of your dataset, make sure you are using the correct version number under project.version. For example, if you created a new version as part of the "Plan B" (training the model with a smaller dataset), verify that the version matches the intended dataset. 

[source,python,role=execute,subs="attributes"]
----
from roboflow import Roboflow

rf = Roboflow(api_key="xxxxxxxxxxxxxxxxx")  # Replace with your API key
project = rf.workspace("workspace").project("user-pass:[<span id="gnumberVal"></span>]-hardhat-detection") # Replace with your workspace and project names

version = project.version(1) # Replace with your version number
dataset = version.download("yolov11")
----



=== Hyperparameter configuration









It's time to prepare our first model prototype, and for that, you'll need to configure the hyperparameters for the first iteration of training.

Model hyperparameters are key configuration settings that define how a machine learning model will be trained. These settings are chosen before training begins and significantly affect the model's performance and efficiency during the training process.

Here are the main hyperparameters you can tune for your YOLO model, along with brief explanations and approximate values to help guide you through the setup:

[NOTE]

The list below is a subset of all the parameters that you can configure. You can find all the https://docs.ultralytics.com/usage/cfg/#train-settings[YOLO training configuration options here], including default values and a short explanation. 

*Training Settings*

* Batch size (`batch`): The https://medium.com/geekculture/how-does-batch-size-impact-your-model-learning-2dd34d9fb1fa[batch size] is the number of training samples used in one forward and backward pass. A larger batch size leads to more stable gradients and will also reduce sustantially the training time but requires more memory. Value will be dependant on your hardware (mainly memory) that you have available in your CPU/GPU, typical values are `16`, `32` or `64`. You can try higher values if your GPU allows it. Take into account that if you are running the training on your CPU and configure a batch size that your container instance size cannot manage,then the Workbench will launch an error while training the model and will ask if you want to restart it.

* Epochs (`epochs`): The https://medium.com/@saiwadotai/epoch-in-machine-learning-understanding-the-core-of-model-training-bfd64bbd5604[Epochs] are the number of complete passes through the entire training dataset. More epochs generally improve model performance but also increase training time and risk of overfitting. Typical values: `50`, `100` (default), `300`. Start with `50` and increase if needed (or just configure `1` epoch if you are running the "Plan B").

* Base YOLO Model (`model`): The base model architecture, which defines the neural network's structure. For YOLO, different versions (e.g., YOLOv4, YOLOv5) or sizes (e.g., YOLOv5s, YOLOv5m) can be selected depending on your requirements. In our project we will base our model in YOLOv11 so you will need to configure `yolo11m.pt`.

* Image Size (`imgsz`): The resolution of the images fed into the model during training. Higher resolutions improve accuracy but increase training time and memory usage. Typical values: `640` (default), `1280`. Start with `640` and increase if your system can handle larger images.

* Patience (`patience`): https://medium.com/@shouke.wei/optimizing-performance-unveiling-the-impact-of-patience-values-on-machine-learning-models-ef1ff3cbdee5 [Patience] is the number of epochs with no improvement in validation performance before the early stopping mechanism kicks in to stop training. This helps prevent overfitting by stopping training early. Typical value is `10` but try to increase the value if you hit the early stopping, to be sure that you are not preventing the training to make your model improve in later epochs.



*Optimization Parameters*

* Optimized (`optimizer`): The algorithm used to minimize the loss function during training. Common optimizers include https://medium.com/@weidagang/demystifying-the-adam-optimizer-in-machine-learning-4401d162cb9e[Adam] and https://mohitmishra786687.medium.com/stochastic-gradient-descent-a-basic-explanation-cbddc63f08e0[SGD (Stochastic Gradient Descent)] being Adam the default. You never know which one could be better so configure either `Adam` or `SGD` and check the results in each case.

* Learning rate (`lr0` and `lrf`): The https://en.wikipedia.org/wiki/Learning_rate[learning rate] controls how quickly the model updates weights during trainicng. Adjusting the learning rate can significantly impact model performance and training time. A learning rate that is too high may cause the model to converge too quickly to a suboptimal solution or fail to converge, while a rate that is too low can slow down training and may result in underfitting. You have two values, the first one is `lr0`, the starting learning rate used at the beginning of the training process and that determines the size of the initial updates made to the model weights during gradient descent. The other value is `lrf`, the Learning Rate Final Multiplier, that is a multiplier that specifies the final learning rate as a fraction of `lr0`, the learning rate gradually decays from `lr0` to `lr0 * lrf` over the course of training. Typical values are `0.01` for both parameters. If the model takes too long to converge, consider increasing the learning rate. However, if you observe sudden fluctuations or jumps in performance, it may indicate the need to reduce the learning rate (ie. `lr0` = `0.001`) to facilitate smoother and more stable convergence.

* Momentum (`momentum`): https://blog.dailydoseofds.com/p/an-intuitive-and-visual-demonstration[Momentum] is a method used in training models to make learning faster and smoother. Instead of just using the current error to update the model, it also remembers the direction it was going in before and if continues in the same directio the learning rate is increased. This helps the model move more steadily, avoid bouncing around too much, and speed up when progress is slow. Default value is `0.937`

* Weight Decay (`weight_decay`): Also known as L2 regularization. https://medium.com/@sujathamudadla1213/weight-decay-in-deep-learning-8fb8b5dd825c[Weight Decay] is a technique that adds a penalty to the loss to prevent overfitting by discouraging large weights. The idea is to encourage the model to keep the weights small, which can lead to simpler, more general models that perform better on unseen data. The default value is `0.0005`.

* Warmups (`warmup_epochs`, `warmup_bias_lr`, `warmup_momentum`): Warmups gradually increase the learning rate during the first few epochs to help the model stabilize before it starts learning aggressively. You have three hyperparameters: `warmup_epochs`, `warmup_bias_lr`, `warmup_momentum`. The `warmup_epochs` (default `0.8`) is the number of steps where the learning rate gradually increases, `warmup_bias_lr` (default `0.1`) controls the initial learning rate for bias parameters during warmup, and `warmup_momentum` (default `3.0`) sets the starting momentum value, all helping to stabilize the model's early training.

* Automatic Mixed Precision (`amp`): Deep Neural Network training has traditionally relied on IEEE single-precision format, however with https://developer.nvidia.com/automatic-mixed-precision[Automatic Mixed Precision], you can train with half precision while maintaining the network accuracy achieved with single precision. It's useful for saving memory and speeding up computations but sometimes its usage cause issues with certain GPUs. Defaults to `True`.


*Additional Model Configuration*

* Name (`name`): The name of the experiment or model version. It helps to track and differentiate between different training runs.

* Dataset path (`data`): The path to the dataset used for training. This includes both training and validation datasets.

* Device used (`device`): The device used for training. Specify whether you are using a CPU or GPU. If using GPU, make sure it's set to cuda.


Besides the hyperparameters above, you can also introduce Data Augmentation settings (additional to the Data Augmentation that you could have applied into your Dataset during the xref:ai-specialist-01-data.adoc[Data Management] section). Check below the options that you have and the default values. 

[NOTE]

If you plan to introduce additional Data Augmentation be sure that you set 'augment` to `True` in order to apply these configurations.

[source,python,subs="attributes"]
----
    # Data augmentation settings
    'augment': True,
    'hsv_h': 0.015,  # HSV-Hue augmentation
    'hsv_s': 0.7,    # HSV-Saturation augmentation
    'hsv_v': 0.4,    # HSV-Value augmentation
    'degrees': 10,    # Image rotation (+/- deg)
    'translate': 0.1, # Image translation
    'scale': 0.3,    # Image scale
    'shear': 0.0,    # Image shear
    'perspective': 0.0,  # Image perspective
    'flipud': 0.1,   # Flip up-down
    'fliplr': 0.1,   # Flip left-right
    'mosaic': 1.0,   # Mosaic augmentation
    'mixup': 0.0,    # Mixup augmentation
----

Now that you’re familiar with the configuration parameters, the goal of this code block is to define and configure a variable (`CONFIG`) that consolidates all your tuning adjustments (other than defaults).

[source,python,subs="attributes"]
----
CONFIG = {
    'var1': 'value1',
    'var2': 'value2',
    ...
    ...
    ...
    'varn': valuen,
}
----

Make your initial guesses for the hyperparameter values for the first model training (next code block). Then, iteratively come back to this code block and adjust and fine-tune these values, retraining the model each time, with the goal of achieving improved performance.


=== Model Training





explain knowledge transfer






check number of epoch



=== Model Evaluation





=== (optional) Model Export





=== Store the Model



















image::ai-build-protomodels.png[]














TEST DATA SPLIT !!!!!A











== Solution and Next Steps




push changes