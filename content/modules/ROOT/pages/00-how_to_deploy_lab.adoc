= How to deploy the lab

== OpenShift

This workshop requires an OpenShift cluster. You might already have one or, if you have access to the Red Hat Demo Platform, you can order either the "RHOAI on OCP on AWS with NVIDIA GPUs" lab, if you want or not GPUs, or the "Red Hat OpenShift Container Platform Cluster (AWS)" lab if you want an "empty" environment.


=== Lab Baseline

The workshop requires several services and configuration (baseline) in OpenShift. You can prepare your environment easily by just importing some YAMLs in your environment:

[example]
====

1- Navigate to OpenShift Console (admin user): {openshift-console}

2- Click on the `+` sign and import the bootstrap-lap YAML. https://github.com/luisarizmendi/workshop-object-detection-rhde/tree/main/deployment/openshift/bootstrap-lab[Choose the right YAML file depending on the platform where you are deploying]. If using the Demo Platform, choose `bootstrap-lab-rhpds-nvidia.yaml` if you are using the "RHOAI on OCP on AWS with NVIDIA GPUs" lab as a base, or the `bootstrap-lab-empty.yaml` if using the "Red Hat OpenShift Container Platform Cluster (AWS)" lab.

image::lab-bootstrap.png[]

3- When the Argo CD access is ready, log in using the admin credentials ().

image::lab-argo.png[]

4- Wait until all Applications are in Synced state (`OutofSync` in `common` application is not a problem)

image::lab-argo-finish.png[]
====


=== Workshop Guide deployment

If you want to have your own Workshop Guide first you need to prepare the values for the guide variables. The default values can be found in the https://github.com/luisarizmendi/workshop-object-detection-rhde/blob/main/content/antora.yml[`antora.yaml` file]. 

Once you know the values that you want to apply to the guide, you can deploy the guide using a Kubernetes Job that runs Helm.


[example]
====
In order to deploy the guide you will need to:

1- Navigate to the OpenShift Web Console

2- Click the `+` icon on the top right corner

3- Copy and paste the following YAML file chaning the values according to your needs

----
apiVersion: batch/v1
kind: Job
metadata:
  name: bootstrap-showroom
  namespace: openshift-gitops
spec:
  template:
    spec:
      serviceAccountName: openshift-gitops-argocd-application-controller
      containers:
      - name: bootstrap-showroom
        image: quay.io/luisarizmendi/helm-cli:latest  
        command: ["/bin/sh", "-c"]
        args:
        - |
          export HOME=/tmp  # Fix permission issues

          NAMESPACE="showroom"

          echo "Creating values.yaml..."
          cat <<EOF > /tmp/values.yaml
          # Common
          git-workshop-url: https://github.com/luisarizmendi/workshop-object-detection-rhde
          openshift-console: https://console-openshift-console.apps.cluster-np6lk.np6lk.sandbox2077.opentlc.com/
          openshift-api: https://api.cluster-np6lk.np6lk.sandbox2077.opentlc.com:6443
          openshift-user-base: user
          openshift-password-base: redhat
          gitea-server: gitea.apps.cluster-np6lk.np6lk.sandbox2077.opentlc.com
          container-registry-gitea: 192.168.1.100
          container-registry-gitea-user: gitea
          container-registry-gitea-pass: gitea
          shared-nvidia-ip: 192.168.1.2
          shared-nvidia-user: admin
          shared-nvidia-pass: R3dh4t1!
          gateway-dns-dhcp-openwrt: http://192.168.1.1
          # Platform
          device-ip-base: 192.168.100.1
          device-username: admin
          device-password: secret
          openshift-ai: https://rhods-dashboard-redhat-ods-applications.apps.cluster-np6lk.np6lk.sandbox2077.opentlc.com/
          flightctl-ui: https://flightui-flightctl.apps.cluster-np6lk.np6lk.sandbox2077.opentlc.com/
          flightctl-api: https://flightapi-flightctl.apps.cluster-np6lk.np6lk.sandbox2077.opentlc.com/
          flightctl-user-basename: flightctluser
          flightctl-password: secretflightctl
          registry-local-url: http://192.168.100.200/workshop/
          # AI
          minio-ui: https://minio-ui-minio.apps.cluster-np6lk.np6lk.sandbox2077.opentlc.com
          minio-api: https://minio-api-minio.apps.cluster-np6lk.np6lk.sandbox2077.opentlc.com
          minio-user-base: user
          minio-password-base: redhat
          registry-url: https://quay.io/user/luisarizmendi/
          EOF

          echo "Ensuring the project exists..."
          if ! /usr/bin/oc get project $NAMESPACE >/dev/null 2>&1; then
            /usr/bin/oc new-project $NAMESPACE
          fi

          echo "Fetching cluster domain..."
          clusterdomain_apps=$(/usr/bin/oc get ingresses.config.openshift.io cluster -o jsonpath='{.spec.domain}')

          echo "Running Helm template..."
          helm repo add larizmen-charts https://raw.githubusercontent.com/luisarizmendi/helm-chart-repo/main/packages
          helm repo update

          helm template showroom larizmen-charts/showroom-single-pod --namespace=${NAMESPACE}  \
          --set deployer.domain=${clusterdomain_apps} \
          --set-file content.user_data=/tmp/values.yaml \
          --set content.repoUrl=https://github.com/luisarizmendi/workshop-object-detection-rhde \
          --set general.guid=1 \
          --set-string content.contentOnly="true" \
          | /usr/bin/oc apply -f -

          echo "Environment ready!"
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - "ALL"
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
      restartPolicy: Never
  backoffLimit: 1
----

4- Click "Create"
====

Once you create the object, the guide will be deployed in a new the `showroom` OpenShift project. You will find in that project a route pointing to the guide that will be available as soon as the showroom POD is running.


== Gitea
preparing the environment Gitea:
$ ansible-playbook playbook.yml -i inventory 

== FlighCTL

  # git clone repo
  # make sure you have the following packaes installed: git, make, and go (>= 1.21), openssl, openssl-devel, podman-compose
  # sudo dnf install git make golang openssl openssl-devel podman-compose
  # ensure podman socket is enabled : systemctl --user enable --now podman.socket
  # build repo: make build
  # install kind:
  #[ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.26.0/kind-linux-amd64 && chmod +x ./kind && sudo mv ./kind /usr/local/bin/kind
  # install kubectl: https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-binary-with-curl-on-linux
  # install helm: https://helm.sh/docs/intro/install/#from-script
  # deploy using helm with kind local cluster (it includes installing helm)
  # make deploy
  # CANT USE LOCAL METHOD AS IT DOESN'T INCLUDE UI
  # install on cluster with acm
  # helm upgrade --install --version=0-latest     --namespace flightctl --create-namespace     flightctl oci://quay.io/flightctl/charts/flightctl     --values content/modules/ROOT/pages/scripts/environment/values.yaml
  # fix redis permission by patching ss with this user and group id: 1000860000
  # install flighctl cli https://github.com/flightctl/flightctl/blob/main/docs/user/getting-started.md#installing-the-flight-control-cli
  # login into flightctl
  # $ flightctl login https://api.flightctl.apps.my.lmf.openshift.es/  --insecure-skip-tls-verify --token=sha256~CGM1m_RbqBqS1bbNdakdGVRU6-2aRZlwzlexZLpVQ3Y
  # now you can get the devices registered with
  # flightctl get devices

== DNS - DHCP - Router Openwrt
using this guide https://openwrt.org/docs/guide-user/installation/openwrt_x86 I'll reuse a device at home






