= Image Baking

So by now you should already be familiar with building a standardized OS using Image Mode, but here is a recap just in case.

Image mode is a new approach to operating system (OS) deployment that lets users build, deploy, and manage Red Hat Enterprise Linux as a bootc container image. 
Bootc is the original project on which this is built on https://containers.github.io/bootc/intro.html[*Bootc*].

Why was this project started at all? 
The original Docker container model of using "layers" to model applications has been extremely successful. This project aims to apply the same technique for bootable host systems - using standard OCI/Docker containers as a transport and delivery format for base operating system updates.

Every tool and technique that you already know and use for creating application base images can now apply to the host Linux OS.

This clearly reduces complexity across the enterprise by letting development, operations, and solution providers use the same container-native tools and techniques to manage everything from applications to the underlying OS. 
Use can use Image Mode for RHEL to build, test, and deploy operating systems by using the same tools and techniques as application containers. Image mode for RHEL is available by using the registry.redhat.io/rhel9/rhel-bootc bootc image. 
The RHEL bootc images differ from the existing application Universal Base Images (UBI) in that they contain additional components necessary to boot that were traditionally excluded, such as, kernel, initrd, boot loader, firmware, among others. 

One of the most obvious ways to use a bootable container as an operating system is to build it into a disk image. The new https://github.com/osbuild/bootc-image-builder[*Bootc Image Builder*] project is a tool that does exactly that. Depending on your needs, it can generate different types of disk images, including:

* raw: Common image format used by many virtualization tools
* qcow: An image format primarily used by QEMU
* iso: For creating bootable media using tools like Balena Etcher
* vmdk: For creating VMs using VMWare Workstation or ESXi
* ami: For creating virtual servers on Amazon EC2

These types of disk images provide lots of options to deploy bootable containers, from booting from a USB drive, starting a virtual machine, or deploying virtual servers to the cloud.

The benefits of using containers do not end here. These are some of the most important ones: 

*   Container images are easier to understand and use than other image formats and are fast to build
*   Consolidate process, infrastructure, and release artifacts and reuse the same tooling available for distributing applications
*   Immutable updates: just as containerized applications are updated in an immutable way, with image mode for RHEL, the operating system is also. 
*   Portability across hybrid cloud environments across physical, virtualized, cloud, and edge environments. 
*   Building container images: you can configure your operating system at a build time by modifying the Containerfile. You can automate the build process by using CI/CD pipelines. 
*   Versioning, mirroring, and testing container images: you can version, mirror, introspect, and sign your derived bootc image by using the container tools of your choice. 
*   Updating your operating system: the system supports in-place transactional updates with rollback after deployment. Automatic updates are on by default. As the updates are transactional, a reboot is required to apply them. 

In the following sections we will go through the following phases: 

image::platform-image-mode-phases.png[]

== Image Build
Since bootc image is available both for x86-64 and aarch64 architectures we are able to leverage this technology on the Nvidia Orin Nano device in front of you. 

The Nvidia device has already been flashed to the latest stable and supported by Red Hat JetPack (6.0). Nvidia JetPack provides a full development environment for the Jetson platform including Jetson Linux with BSP, kernel, RootFS, etc., the NVIDIA AI stack with hardware accelerated libraries, SDKs, and frameworks, and Jetson Platform Services for application development.

A RHEL is already installed on the Nvidia device but it is RPM based. 
Since you don't know yet the IP address or the credentials to the Nvidia Device and you (as Platform Specialist) would like to standardize and simplify platform building and deployment, you are going to leverage bootc.

We will use as starting point a prebuilt bootc-compatible image and we will build on it, with the objective to include a way to allow us to reach the Nvidia Device remotely, without plugging on to the console or DisplayPort.

You will find this container image on the local container registry (based on Gitea) here: {container-registry-gitea}/{container-registry-gitea-user}/nvidia:0.0.1

#Make sure to pull the container image with `sudo` since the last step of building the bootc image requires `sudo` permissions.#

Let's analyze the base image provided:

[source,docker]
----
FROM registry.gitlab.com/redhat/rhel/sst/orin-sidecar/nvidia-jetson-sidecar/rhel-9.4:36.3.1-20240516220919

ARG USHIFT_VER=4.17
RUN dnf config-manager \
        --set-enabled rhocp-${USHIFT_VER}-for-rhel-9-$(uname -m)-rpms \
        --set-enabled fast-datapath-for-rhel-9-$(uname -m)-rpms
RUN dnf install -y firewalld microshift && \
    systemctl enable microshift && \
    dnf clean all

# Create a default 'redhat' user with the specified password.
# Add it to the 'wheel' group to allow for running sudo commands.
ARG USER_PASSWD
RUN if [ -z "${USER_PASSWD}" ] ; then \
        echo USER_PASSWD is a mandatory build argument && exit 1 ; \
    fi
RUN useradd -m -d /var/home/redhat && \
    usermod -G video redhat && \
    echo "redhat:${USER_PASSWD}" | chpasswd

# Mandatory firewall configuration
RUN firewall-offline-cmd --zone=public --add-port=22/tcp && \
    firewall-offline-cmd --zone=trusted --add-source=10.42.0.0/16 && \
    firewall-offline-cmd --zone=trusted --add-source=169.254.169.1

# Create a systemd unit to recursively make the root filesystem subtree
# shared as required by OVN images
RUN cat > /etc/systemd/system/microshift-make-rshared.service <<'EOF'
[Unit]
Description=Make root filesystem shared
Before=microshift.service
ConditionVirtualization=container
[Service]
Type=oneshot
ExecStart=/usr/bin/mount --make-rshared /
[Install]
WantedBy=multi-user.target
EOF
RUN systemctl enable microshift-make-rshared.service
----

In the Containerfile in order: 

* we are basing image of RHEL 9.4
* we are including Microshift as well, as we will be using it in the last part of this lab to deploy applications
* we are creating a `redhat` user, and adding that user to the `video` group, since it will allow him to use GPU resources
* we are afterwards opening the needed firewall ports
* we are finally making root filesystem shared as required by OVN containers


What we are going to add to the image is:

. add administrative rights to the `redhat` user, change the user password and enable passwordless escalation
. install and enable cockpit interface for remote graphical management
. a script to notify you of the acquired IP address
. a customization to also print the IP address on the console

Since the first 2 points should be fairly easy to include, I'll leave them as exercise for the reader, but you can find them as snippets below.

.Click to reveal it
[%collapsible]
====
[source,docker]
----
...
RUN usermod -aG wheel redhat
RUN echo "redhat:R3dh4t1!" | chpasswd
RUN echo "%wheel        ALL=(ALL)       NOPASSWD: ALL" > /etc/sudoers.d/wheel-sudo
...
RUN dnf -y install cockpit && dnf -y clean all && systemctl enable cockpit.socket
...
----
====

For the third point, we are going to use the following script and add that as a service in our Containerfile so that it is started at boot time.

[source,docker]
----
RUN cat > /var/tmp/network-info.sh <<EOF
#!/bin/bash
sleep 5
conn_name=\$(nmcli -t -f NAME con show | head -n 1)
device_name=\$(nmcli -t -f GENERAL.DEVICES con show "\$conn_name" | head -n 1 | cut -d: -f2)
IP_ADDRESS=\$(nmcli -t -f IP4.ADDRESS con show "\$conn_name" | head -n 1 | cut -d: -f2 | cut -d/ -f1)
MAC_ADDRESS=\$(nmcli -g GENERAL.HWADDR device show "\$device_name" | tr -d '\\')
MAC_ADDRESS_FORMAT=\$(echo "\$MAC_ADDRESS" | tr -d ':')


if [ -z "\$IP_ADDRESS" ] || [ -z "\$MAC_ADDRESS" ] || [ -z "\$USER" ]; then
    echo "One or more required variables are empty. Script failed."
    exit 1
fi


JSON="{ \
\"body\":\"\$IP_ADDRESS\", \
\"title\": \"GROUP-NAME-CHANGE-ME\" \
}"

echo "\$JSON"

/usr/bin/curl -k -H 'Content-Type: application/json' -u {container-registry-gitea-user}:{container-registry-gitea-pass} --data "\$JSON" http://{container-registry-gitea}/api/v1/repos/{container-registry-gitea-user}/inventories/issues
EOF

RUN chmod +x  /var/tmp/network-info.sh

RUN cat > /etc/systemd/system/network-info.service <<EOF
[Unit]
Description=Register Network Info onto Gitea
After=network.target
After=connect-wifi.service
ConditionPathExists=!/var/tmp/net-info-registered

[Service]
Type=simple
ExecStart=/bin/bash -c 'while true; do /var/tmp/network-info.sh && /usr/bin/touch /var/tmp/net-info-registered && break; done'

[Install]
WantedBy=default.target
EOF

RUN systemctl enable network-info.service
----


For the last point we are going to leverage the https://man7.org/linux/man-pages/man5/issue.5.html[issue] file in Linux, and add the following:

[source,bash]
----
IP: \4
----


NOTE: To build this new image you are going to use the shared Nvidia Device available on this network at the following address {shared-nvidia-ip} since we are assuming that you don't know yet how to reach the Nvidia Device you have in front of you. 
Username and password for the shared Nvidia Device are {shared-nvidia-user}:{shared-nvidia-pass}. 

WARNING: Watch out for using your assigned Nvidia Device to try and build the next iteration of the Device Image! To do that you would need a fully subscribed RHEL 9 System, with the same system architecture as the target device (so *aarch64*). So for this and the following step you would need to use the shared Nvidia Device mentioned above.  

You can find the complete Containerfile here:


.Click to reveal it
[%collapsible]
====
[source,docker]
----
FROM {container-registry-gitea}/{container-registry-gitea-user}/nvidia:0.0.1

RUN dnf -y install cockpit && dnf -y clean all && systemctl enable cockpit.socket

# Modifying default user
RUN usermod -aG wheel redhat
RUN echo "redhat:R3dh4t1!" | chpasswd
RUN echo "%wheel        ALL=(ALL)       NOPASSWD: ALL" > /etc/sudoers.d/wheel-sudo

# Network info script
RUN cat > /var/tmp/network-info.sh <<EOF
#!/bin/bash
sleep 5
conn_name=\$(nmcli -t -f NAME con show | head -n 1)
device_name=\$(nmcli -t -f GENERAL.DEVICES con show "\$conn_name" | head -n 1 | cut -d: -f2)
IP_ADDRESS=\$(nmcli -t -f IP4.ADDRESS con show "\$conn_name" | head -n 1 | cut -d: -f2 | cut -d/ -f1)
MAC_ADDRESS=\$(nmcli -g GENERAL.HWADDR device show "\$device_name" | tr -d '\\')
MAC_ADDRESS_FORMAT=\$(echo "\$MAC_ADDRESS" | tr -d ':')


if [ -z "\$IP_ADDRESS" ] || [ -z "\$MAC_ADDRESS" ] || [ -z "\$USER" ]; then
    echo "One or more required variables are empty. Script failed."
    exit 1
fi


JSON="{ \
\"body\":\"\$IP_ADDRESS\", \
\"title\": \"GROUP-NAME-CHANGE-ME\" \
}"

echo "\$JSON"

/usr/bin/curl -k -H 'Content-Type: application/json' -u {container-registry-gitea-user}:{container-registry-gitea-pass} --data "\$JSON" http://{container-registry-gitea}/api/v1/repos/{container-registry-gitea-user}/inventories/issues
EOF

RUN chmod +x  /var/tmp/network-info.sh

RUN cat > /etc/systemd/system/network-info.service <<EOF
[Unit]
Description=Register Network Info onto Gitea
After=network.target
After=connect-wifi.service
ConditionPathExists=!/var/tmp/net-info-registered

[Service]
Type=simple
ExecStart=/bin/bash -c 'while true; do /var/tmp/network-info.sh && /usr/bin/touch /var/tmp/net-info-registered && break; done'

[Install]
WantedBy=default.target
EOF

RUN systemctl enable network-info.service

# Priting IPv4 address on console
RUN echo "IP: \4" >> /etc/issue

# Mandatory firewall configuration
RUN firewall-offline-cmd --zone=public --add-port=9090/tcp
EXPOSE 9090
----
====

Given the complete Containerfile, we are going to build a new container image and then push it to the central Gitea Container Registry.

Make sure to create a new folder on the shared Nvidia Device with your group name and copy the created Containerfile there.  
You can now build the new Container image like this (assuming there is just one Containerfile in the working directory from where you are building and that you are already logged into the Container Registry):

[source,bash]
----
$ sudo podman build -t {container-registry-gitea}/group-name-change-me/nvidia:0.0.2 .
----


== Image Deploy
Now that you have built the "refreshed" container image we need to deploy it to your personal Nvidia Device sitting on your desk.

For that we need to use the `bootc-image-builder` as highlighted https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/using_image_mode_for_rhel_to_build_deploy_and_manage_operating_systems/creating-bootc-compatible-base-disk-images-with-bootc-image-builder_using-image-mode-for-rhel-to-build-deploy-and-manage-operating-systems#creating-bootc-compatible-base-disk-images-with-bootc-image-builder_using-image-mode-for-rhel-to-build-deploy-and-manage-operating-systems[here].  

In our case we would be creating an ISO omage with an embedded Kickstart file (an example is found https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/using_image_mode_for_rhel_to_build_deploy_and_manage_operating_systems/creating-bootc-compatible-base-disk-images-with-bootc-image-builder_using-image-mode-for-rhel-to-build-deploy-and-manage-operating-systems#using-bootc-image-builder-to-build-iso-images-with-a-kickstart-file_creating-bootc-compatible-base-disk-images-with-bootc-image-builder[here])
command to create new iso file
use usb drive and burn it
TODO : disable autoupdate for bootc!



== Image Rollout
boot up nvidia with device
remote access


.SOLUTION
====
pointer to the image built already with script
and iso generate uploaded to httpd
====


Move on to xref:platform-specialist-02-device-onboarding.adoc[Device Onboarding].

to build bootc image for nvidia platform which includes microshift started from image http://quay.io/hgeaydem/microshift-4.17-jetson-bootc:latest
going to use the same nvidia platform to build the iso image

BUILDING X86 IMAGE FOR TESTING
build the container image like this (remember to authenticate to registry.redhat.io first!):
$ sudo podman build -t localhost/rhel9-baseline:0.0.1 .
push image to registry:
$ sudo podman push localhost/rhel9-baseline:0.0.1 osbuild.lmf.openshift.es:7443/lucamaf/rhel9-baseline:0.0.1
ISSUE : finally discovered that bootc doesn't support yet pulling from insecure registry: https://github.com/containers/bootc/issues/461
trying with quay.io
$ sudo podman push localhost/rhel9-baseline:0.0.1 quay.io/luferrar/rhel9-baseline:0.0.1
build iso:
$ sudo podman run     --rm     -it     --privileged    --pull=newer     --security-opt label=type:unconfined_t     -v /var/lib/containers/storage:/var/lib/containers/storage     -v $(pwd)/config.toml:/config.toml     -v $(pwd)/output:/output     registry.redhat.io/rhel9/bootc-image-builder:latest     --type iso  --tls-verify=false    --config /config.toml   osbuild.lmf.openshift.es:7443/lucamaf/rhel9-baseline:0.0.1

turned off automatic updates:
https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/using_image_mode_for_rhel_to_build_deploy_and_manage_operating_systems/managing-rhel-bootc-images#turning-off-automatic-updates_managing-rhel-bootc-images

create the updated version with flightctl included

build new image
$ sudo podman build -t localhost/rhel9-baseline:0.0.1 -f Containerfile-agent

push image to local registry
$  sudo podman push localhost/rhel9-baseline:0.0.1 osbuild.lmf.openshift.es:7443/lucamaf/rhel9-baseline:0.0.1

check that the device is pointing to local registry
$ sudo bootc status

check update available
$ sudo bootc upgrade --check
Update available for: docker://osbuild.lmf.openshift.es:7443/lucamaf/rhel9-baseline:0.0.1
  Version: 9.20250109.0
  Digest: sha256:77ff6ec1713ab776d691bb8cfe131543d76a4399c7cc0fd75f01821c1941a2b2
Total new layers: 78    Size: 1.8 GB
Removed layers:   0     Size: 0 bytes
Added layers:     3     Size: 251.7 MB

upgrade device
$ bootc upgrade

# TODO at the moment the agent doesn't seem to work out of the box, but after creating the folder /var/lib/flighctl it seems to start

approve device in the GUI and add some labels

create a fleet in the GUI

to install bootc image builder follow this
https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/using_image_mode_for_rhel_to_build_deploy_and_manage_operating_systems/creating-bootc-compatible-base-disk-images-with-bootc-image-builder_using-image-mode-for-rhel-to-build-deploy-and-manage-operating-systems#installing-bootc-image-builder_creating-bootc-compatible-base-disk-images-with-bootc-image-builder

to deregister a device delete agent key in /var/lib/flightctl/certs

## NVIDIA
build an iso image with kickstart file embedded
https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/using_image_mode_for_rhel_to_build_deploy_and_manage_operating_systems/creating-bootc-compatible-base-disk-images-with-bootc-image-builder_using-image-mode-for-rhel-to-build-deploy-and-manage-operating-systems#using-bootc-image-builder-to-build-iso-images-with-a-kickstart-file_creating-bootc-compatible-base-disk-images-with-bootc-image-builder
$ sudo podman run     --rm     -it     --privileged     --pull=newer     --security-opt label=type:unconfined_t     -v /var/lib/containers/storage:/var/lib/containers/storage     -v $(pwd)/config.toml:/config.toml     -v $(pwd)/output:/output     registry.redhat.io/rhel9/bootc-image-builder:latest     --type iso     --config /config.toml   quay.io/hgeaydem/microshift-4.17-jetson-bootc

copy iso to usb using dd
$ sudo dd if=output/bootiso/install.iso of=/dev/sda bs=4M status=progress oflag=sync

booted and added openshift-pull-secret to /etc/crio on the device

create a new version of the image with a system service to copy kubeconfig in the admin home directory
how to switch from local ostree repo to registry:


LAB PREPARATION:
installed gitea as container registry with the following playbook and inventory
check the playbook and default conf file from nginx for configuration (add gitea as address in etc hosts file)
IDEA can pushg issue to gitea registry at boot time with hostname and ip address

HW:  add neofetch to the containerfile to display Ip address as MOTD
configure neofetch like this: https://github.com/dylanaraps/neofetch/wiki/Getting-Started

add flighctl agent following this:
https://github.com/flightctl/flightctl/blob/main/docs/user/getting-started.md#building-a-bootable-container-image-including-the-flight-control-agent

command to be executed:

Retrieve the agent configuration with enrollment credentials by running:

$ flightctl certificate request --signer=enrollment --expiration=365d --output=embedded > config.yaml

The returned config.yaml should look similar to this:

$ cat config.yaml
enrollment-service:
  service:
    server: https://agent-api.flightctl.127.0.0.1.nip.io:7443
    certificate-authority-data: LS0tLS1CRUdJTiBD...
  authentication:
    client-certificate-data: LS0tLS1CRUdJTiBD...
    client-key-data: LS0tLS1CRUdJTiBF...
  enrollment-ui-endpoint: https://ui.flightctl.127.0.0.1.nip.io:8081

Create a Containerfile with the following content:

$ cat Containerfile

FROM quay.io/centos-bootc/centos-bootc:stream9

RUN dnf -y copr enable @redhat-et/flightctl-dev centos-stream-9-x86_64 && \
    dnf -y install flightctl-agent; \
    dnf -y clean all; \
    systemctl enable flightctl-agent.service

# Optional: to enable podman-compose application support uncomment below”
# RUN dnf -y install epel-release epel-next-release && \
#    dnf -y install podman-compose && \
#    systemctl enable podman.service

ADD agentconfig.yaml /etc/flightctl/config.yaml